{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd22a6e8",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf36a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "from eppy import modeleditor\n",
    "from eppy.modeleditor import IDF\n",
    "import os\n",
    "import pandas as pd \n",
    "import glob\n",
    "import datetime\n",
    "import plotly as plty\n",
    "import plotly.express as px\n",
    "import calendar\n",
    "import numpy as np\n",
    "import re\n",
    "import dash\n",
    "from dash import dcc, html, dash_table\n",
    "from dash.dependencies import Input, Output\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd71edb",
   "metadata": {},
   "source": [
    "# Load energy model and set output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "\n",
    "##Energy model input\n",
    "idf_file=r\"C:\\Users\\Desktop\\EP.idf\"\n",
    "\n",
    "#Energyplus idd file input\n",
    "iddfile =r\"C:\\EnergyPlusV22-1-0\\Energy+.idd\"\n",
    "\n",
    "#Weather file input\n",
    "epwfile=r\"C:\\Users\\Desktop\\WeatherFile\\CAN_ON_TORONTO-INTL-A_6158731_CWEC.epw\"\n",
    "    \n",
    "#Set idd file\n",
    "IDF.setiddname(iddfile)\n",
    "\n",
    "#Make copy of original idf file(optional) \n",
    "idf_original = IDF(idf_file)\n",
    "\n",
    "#Set model object for conversion\n",
    "idf1 = IDF(idf_file)\n",
    "\n",
    "#Set Output File Name\n",
    "output_filename=idf_file.rsplit('\\\\', 1)[-1].rsplit('.', 1)[0]\n",
    "output_filename=output_filename+\"_TRConverted.idf\"\n",
    "\n",
    "#Set Output Path\n",
    "idf_output_folder=r\"C:\\Users\\Desktop\\Output\"\n",
    "\n",
    "print(output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82e0d5",
   "metadata": {},
   "source": [
    "# Input - Define Disruptive Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a0b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Disruptive Event\n",
    "\n",
    "Event_StartYear=\"2023\"\n",
    "\n",
    "# Date format is \"Month/Day\"\n",
    "Event_StartDate=\"7/15\"\n",
    "\n",
    "# Time format is \"hour:minute\"in 24 hour scale - 0:00 to 23:00\n",
    "Event_StartTime=\"06:00\"\n",
    "\n",
    "Event_EndYear=\"2023\"\n",
    "\n",
    "# Date format is \"Month/Day\"\n",
    "Event_EndDate=\"7/23\"\n",
    "\n",
    "# Time format is \"hour:minute\"in 24 hour scale - 0:00 to 23:00\n",
    "Event_EndTime=\"23:00\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02625f49",
   "metadata": {},
   "source": [
    "# Compile disruptive event start and end date/time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977a2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compile_date(Event_StartYear,Event_StartDate,Event_StartTime,Event_EndYear,Event_EndDate,Event_EndTime):\n",
    "######Purpose of this function is to convert user entered date time into variables##########\n",
    "######Inputs of the function are user input event start and end date/time##################\n",
    "######Outputs of this function are variables that will be used in downstream functions######\n",
    "\n",
    "    Event_StartDate_Year=int(Event_StartYear)\n",
    "    Event_StartDate_Month=int(Event_StartDate.split(\"/\")[0])\n",
    "    Event_StartDate_Day=int(Event_StartDate.split(\"/\")[1])\n",
    "    Event_StartDate_Hour=int(Event_StartTime.split(\":\")[0])\n",
    "    Event_StartDate_Minute=int(Event_StartTime.split(\":\")[1])\n",
    "\n",
    "    Event_EndDate_Year=int(Event_EndYear)\n",
    "    Event_EndDate_Month=int(Event_EndDate.split(\"/\")[0])\n",
    "    Event_EndDate_Day=int(Event_EndDate.split(\"/\")[1])\n",
    "    Event_EndDate_Hour=int(Event_EndTime.split(\":\")[0])\n",
    "    Event_EndDate_Minute=int(Event_EndTime.split(\":\")[1])\n",
    "\n",
    "    return(Event_StartDate_Year,Event_StartDate_Month,Event_StartDate_Day,Event_StartDate_Hour,Event_StartDate_Minute,Event_EndDate_Year,Event_EndDate_Month,Event_EndDate_Day,Event_EndDate_Hour,Event_EndDate_Minute)\n",
    "    \n",
    "Event_StartDate_Year,Event_StartDate_Month,Event_StartDate_Day,Event_StartDate_Hour,Event_StartDate_Minute,Event_EndDate_Year,Event_EndDate_Month,Event_EndDate_Day,Event_EndDate_Hour,Event_EndDate_Minute=compile_date(Event_StartYear,Event_StartDate,Event_StartTime,Event_EndYear,Event_EndDate,Event_EndTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca363fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create DateTime object for event start and end date\n",
    "\n",
    "## If the period start and end date of simulation is not filled in, event start/end year needs to be selected based on the weekday that simulation starts, this is to allow the use of date time functions in the following steps\n",
    "\n",
    "RunPeriod_idf=idf1.idfobjects['RunPeriod'][0]\n",
    "\n",
    "if RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Sunday\":\n",
    "    \n",
    "    RunPeriod_Year=2023\n",
    "\n",
    "elif RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Monday\":\n",
    "    \n",
    "    RunPeriod_Year=2018\n",
    "    \n",
    "elif RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Tuesday\":\n",
    "    \n",
    "    RunPeriod_Year=2019\n",
    "    \n",
    "elif RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Wednesday\":\n",
    "    \n",
    "    RunPeriod_Year=2014\n",
    "    \n",
    "elif RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Thursday\":\n",
    "    \n",
    "    RunPeriod_Year=2015\n",
    "    \n",
    "elif RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Friday\":\n",
    "    \n",
    "    RunPeriod_Year=2010\n",
    "    \n",
    "elif RunPeriod_idf.Begin_Year == \"\" and RunPeriod_idf.Day_of_Week_for_Start_Day == \"Saturday\":\n",
    "    \n",
    "    RunPeriod_Year=2011\n",
    "    \n",
    "else:\n",
    "    RunPeriod_Year=int(RunPeriod_idf.Begin_Year)\n",
    "    \n",
    "Event_Start_Datetime = datetime.datetime(RunPeriod_Year,Event_StartDate_Month,Event_StartDate_Day,Event_StartDate_Hour,Event_StartDate_Minute)\n",
    "Event_End_Datetime = datetime.datetime(RunPeriod_Year,Event_EndDate_Month,Event_EndDate_Day,Event_EndDate_Hour,Event_EndDate_Minute)\n",
    "\n",
    "# Create analysis table to indicate whether event start and end date falls under a weekday or weekend\n",
    "\n",
    "EventDate_Analyze_df=pd.DataFrame({'Event Date':[Event_Start_Datetime,Event_End_Datetime]})\n",
    "EventDate_Analyze_df[\"Is_Weekday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(0,4)\n",
    "EventDate_Analyze_df[\"Is_Weekend\"]=EventDate_Analyze_df['Is_Weekday'] == False\n",
    "\n",
    "EventDate_Analyze_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f84efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The tool needs to determine whether the event start and end dates are special days such as holidays, \n",
    "#and if not, which weekday they belong to\n",
    "\n",
    "##Function to translate special day date description in energyplus model into python date time object\n",
    "\n",
    "def RunPeriodSpecial(Special_Date):\n",
    "######Purpose of this function is to translate special day date description in energyplus model into python date time object##########\n",
    "######Input of the function is a special date in energyplus idf file##################################################################\n",
    "######Output of this function is a formatted python date object#######################################################################\n",
    "\n",
    "    WeekDayList={'Monday':0, 'Tuesday':1, 'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5, 'Sunday':6}\n",
    "    MonthList={'January':1, 'February':2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12}\n",
    "    Special_Date=Special_Date\n",
    "    if len(Special_Date.split())==2:\n",
    "         Special_Date_Month=MonthList[str(Special_Date.split(\" \")[0])]\n",
    "         Special_Date_Day=int(Special_Date.split(\" \")[1])\n",
    "         Special_Date_formatted=datetime.date(RunPeriod_Year,Special_Date_Month,Special_Date_Day)             \n",
    "    elif len(Special_Date.split())==4:\n",
    "         Special_Date_NumDay=str(Special_Date.split(\" \")[0])\n",
    "         Special_Date_WeekDay=WeekDayList[str(Special_Date.split(\" \")[1])]\n",
    "         Special_Date_Month=MonthList[str(Special_Date.split(\" \")[3])]\n",
    "        \n",
    "         if len(Special_Date_NumDay)==3:\n",
    "             Special_Date_NumDay=int(Special_Date_NumDay[0])\n",
    "             Special_Date_formatted=calendar.Calendar(Special_Date_WeekDay).monthdatescalendar(RunPeriod_Year, Special_Date_Month)[Special_Date_NumDay][0]\n",
    "         elif Special_Date_NumDay==\"Last\" or Special_Date_NumDay==\"last\":\n",
    "             Special_Date_formatted=calendar.Calendar(Special_Date_WeekDay).monthdatescalendar(RunPeriod_Year, Special_Date_Month)[-1][0]\n",
    "    \n",
    "    return Special_Date_formatted\n",
    "\n",
    "## Function to list all special days in an energyplus model with formatted python datetime\n",
    "\n",
    "def RunPeriodSpecialDays(idf1):\n",
    "######Purpose of this funciton is to list all special days in an energyplus model with formatted python datetime#####################################\n",
    "######Input of the funciton is the energyplus idf file(eppy object)##################################################################################\n",
    "######Output of this funciton is a pandas dataframe with all special days in an energyplus model with formatted python datetime######################\n",
    "    \n",
    "    #Special Days\n",
    "    idf_RunPeriodSpecialDays=idf1.idfobjects[\"RUNPERIODCONTROL:SPECIALDAYS\"]\n",
    "    idf_RunPeriodSpecialDays_len=len(idf_RunPeriodSpecialDays)\n",
    "\n",
    "    ##list all Special Days - Holiday\n",
    "    SpecialDay_list=[]\n",
    "    StartDate_list=[]\n",
    "    Duration_list=[]\n",
    "    SpecialDay_Type_list=[]\n",
    "\n",
    "    SpecialDay_list=[SpecialDay[\"Name\"] for SpecialDay in idf_RunPeriodSpecialDays]\n",
    "    StartDate_list=[SpecialDay[\"Start_Date\"] for SpecialDay in idf_RunPeriodSpecialDays]\n",
    "    Duration_list=[SpecialDay[\"Duration\"] for SpecialDay in idf_RunPeriodSpecialDays]\n",
    "    SpecialDay_Type_list=[SpecialDay[\"Special_Day_Type\"] for SpecialDay in idf_RunPeriodSpecialDays]\n",
    "    \n",
    "    \n",
    "    RunPeriodSpcialDay_list=pd.DataFrame({'Name':SpecialDay_list,'Start_Date':StartDate_list, 'Duration':Duration_list, 'Special_Day_Type':SpecialDay_Type_list})\n",
    "    \n",
    "    RunPeriodSpcialDay_list['Start_Date_Formatted']=pd.to_datetime(RunPeriodSpcialDay_list['Start_Date'].apply(RunPeriodSpecial))\n",
    "    RunPeriodSpcialDay_list['Duration']=pd.to_timedelta(RunPeriodSpcialDay_list['Duration'],'d')\n",
    "    RunPeriodSpcialDay_list['End_Date_Formatted']=RunPeriodSpcialDay_list['Start_Date_Formatted']+RunPeriodSpcialDay_list['Duration']-datetime.timedelta(days=1)\n",
    "    RunPeriodSpcialDay_list['Start_Date_Formatted']=RunPeriodSpcialDay_list['Start_Date'].apply(RunPeriodSpecial)\n",
    "    RunPeriodSpcialDay_list['End_Date_Formatted']=RunPeriodSpcialDay_list['End_Date_Formatted'].map(datetime.datetime.date) \n",
    "\n",
    "    return(RunPeriodSpcialDay_list)\n",
    "\n",
    "RunPeriodSpecialDay_df=RunPeriodSpecialDays(idf1)\n",
    "\n",
    "## Steps to check whether the start of the disruptive event starts/ends on a special day.This is relevant in constructing schedule for disruptive events\n",
    "\n",
    "RunPeriodSpecialDay_StartDateCheck_df=RunPeriodSpecialDay_df.loc[((Event_Start_Datetime.date()>=RunPeriodSpecialDay_df['Start_Date_Formatted']) & (Event_Start_Datetime.date()<=RunPeriodSpecialDay_df['End_Date_Formatted']))]\n",
    "RunPeriodSpecialDay_EndDateCheck_df=RunPeriodSpecialDay_df.loc[((Event_End_Datetime.date()>=RunPeriodSpecialDay_df['Start_Date_Formatted']) & (Event_End_Datetime.date()<=RunPeriodSpecialDay_df['End_Date_Formatted']))]\n",
    "\n",
    "## Check if start date is on a holiday, if so, list the holiday name\n",
    "RunPeriodSpecialDay_DateCheck=[0,0]\n",
    "if RunPeriodSpecialDay_StartDateCheck_df.empty:\n",
    "    RunPeriodSpecialDay_DateCheck[0] = False\n",
    "    \n",
    "elif RunPeriodSpecialDay_StartDateCheck_df.iloc[0]['Special_Day_Type'] == \"CustomDay1\":\n",
    "    \n",
    "    RunPeriodSpecialDay_DateCheck[0] = \"CustomDay1\"\n",
    "    \n",
    "elif RunPeriodSpecialDay_StartDateCheck_df.iloc[0]['Special_Day_Type'] == \"CustomDay2\":\n",
    "    \n",
    "    RunPeriodSpecialDay_DateCheck[0] = \"CustomDay2\"\n",
    "    \n",
    "else: \n",
    "    RunPeriodSpecialDay_DateCheck[0] = RunPeriodSpecialDay_StartDateCheck_df.iloc[0]['Name']\n",
    "\n",
    "## Check if end date is on a holiday, if so, list the holiday name\n",
    "if RunPeriodSpecialDay_EndDateCheck_df.empty:\n",
    "    RunPeriodSpecialDay_DateCheck[1] = False\n",
    "    \n",
    "elif RunPeriodSpecialDay_EndDateCheck_df.iloc[0]['Special_Day_Type'] == \"CustomDay1\":\n",
    "    \n",
    "    RunPeriodSpecialDay_DateCheck[1] = \"CustomDay1\"\n",
    "    \n",
    "elif RunPeriodSpecialDay_EndDateCheck_df.iloc[0]['Special_Day_Type'] == \"CustomDay2\":\n",
    "    \n",
    "    RunPeriodSpecialDay_DateCheck[1] = \"CustomDay2\"\n",
    "    \n",
    "else: \n",
    "    RunPeriodSpecialDay_DateCheck[1] = RunPeriodSpecialDay_EndDateCheck_df.iloc[0]['Name']\n",
    "\n",
    "EventDate_Analyze_df[\"Is_Holiday\"]=RunPeriodSpecialDay_DateCheck\n",
    "\n",
    "# Steps to check which day of the week the disruptive event start/end date begins on.\n",
    "\n",
    "EventDate_Analyze_df[\"Is_Weekend\"]=EventDate_Analyze_df['Is_Weekday'] == False\n",
    "\n",
    "EventDate_Analyze_df[\"Is_Monday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(0,0)\n",
    "EventDate_Analyze_df[\"Is_Tuesday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(1,1)\n",
    "EventDate_Analyze_df[\"Is_Wednesday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(2,2)\n",
    "EventDate_Analyze_df[\"Is_Thursday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(3,3)\n",
    "EventDate_Analyze_df[\"Is_Friday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(4,4)\n",
    "EventDate_Analyze_df[\"Is_Saturday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(5,5)\n",
    "EventDate_Analyze_df[\"Is_Sunday\"]=EventDate_Analyze_df['Event Date'].dt.dayofweek.between(6,6)\n",
    "\n",
    "\n",
    "#The resulted analysis dataframe can be used to inform schedule selection\n",
    "EventDate_Analyze_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f500f",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cffde8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ListOfSchedules(idf1):\n",
    "\n",
    "######Purpose of this function is to list all the schedules in a energyplus input file(idf file)##########\n",
    "######Input of the function is an idf file loaded as an IDF object using the eppy library#################\n",
    "######Output of this function is a pandas dataframe with all schedule#####################################\n",
    "    \n",
    "#List all schedules in idf file\n",
    "    matching = [s for s in idf1.idfobjects if s.startswith(\"SCHEDULE:\")]\n",
    "\n",
    "# initialize storage array for schedules in idf file\n",
    "    idf_schedules=[]\n",
    "\n",
    "    for match in matching:\n",
    "        idf_schedules.extend(idf1.idfobjects[match])\n",
    "\n",
    "    schedule_name=[idf_schedule[\"Name\"] for idf_schedule in idf_schedules]\n",
    "    schedule_type=[idf_schedule[\"key\"] for idf_schedule in idf_schedules]\n",
    "    \n",
    "    schedule_limit_type=[]\n",
    "    \n",
    "    for idf_schedule in idf_schedules:\n",
    "        try:\n",
    "            schedule_limit_type.append(idf_schedule[\"Schedule_Type_Limits_Name\"])\n",
    "        except:\n",
    "            schedule_limit_type.append(\"\")\n",
    "\n",
    "    schedule_list=pd.DataFrame({'Name':schedule_name,'Type':schedule_type, 'Limit Type':schedule_limit_type})\n",
    "    \n",
    "    \n",
    "    return(schedule_list)\n",
    "\n",
    "\n",
    "def NewScheduleType_HVAC_Control(idf1):\n",
    "    \n",
    "######Purpose of this function is to create a new schedule type - Control#################################\n",
    "######Input of the function is an idf file loaded as an IDF object using the eppy library#################\n",
    "######Output of this function is the IDF object with the new schedule type added##########################\n",
    "    \n",
    "    #Define Schedule Type - Control - in case it doesn't already exist or named differently\n",
    "    #-! Need to add logic to check that schedul type does not already exist\n",
    "    \n",
    "    newobject = idf1.newidfobject(\"SCHEDULETypeLimits\")\n",
    "    newobject.Name='Control'\n",
    "    newobject.Lower_Limit_Value=\"0\"\n",
    "    newobject.Upper_Limit_Value=\"4\"\n",
    "    newobject.Numeric_Type=\"Discrete\"\n",
    "\n",
    "    print(newobject)\n",
    "    \n",
    "    return(idf1)\n",
    "\n",
    "def NewSchedule_Uncontrolled_HVAC(idf1):\n",
    "    \n",
    "######Purpose of this function is to create a new schedule - Uncontrolled HVAC - turning off HVAC for full year##########\n",
    "######Input of the function is an idf file loaded as an IDF object using the eppy library################################\n",
    "######Output of this function is the IDF object with the new schedule added##############################################\n",
    "    \n",
    "    # Create Schedule - Uncontrolled_HVAC \n",
    "\n",
    "    newobject = idf1.newidfobject(\"SCHEDULE:COMPACT\")\n",
    "    newobject.Name='HVAC_Uncontrolled_WholeYear'\n",
    "    newobject.Schedule_Type_Limits_Name=\"Control\"\n",
    "    newobject.Field_1=\"Through:12/31\"\n",
    "    newobject.Field_2=\"For: AllDays\"\n",
    "    newobject.Field_3=\"Until: 24:00\"\n",
    "    newobject.Field_4=\"0\"\n",
    "    \n",
    "    print(\"New Schedule added to idf:\")\n",
    "    print(newobject)\n",
    "    \n",
    "    return(idf1)\n",
    "\n",
    "\n",
    "def NewSchedule_PowerOff(idf1):\n",
    "    \n",
    "    # Purpose of this function is to create \n",
    "    #    a schedule that is always off\n",
    "    \n",
    "    # Input of this function is an energyplus input file(idf) \n",
    "    #    loaded into an eppy IDF object\n",
    "    \n",
    "    # Output of this function(function returns), \n",
    "    #    is the eppy IDF object with the added schedule\n",
    "    \n",
    "    # A message is also displayed to notify users that a new \n",
    "    #    schudle is added to the idf model,and the details of the \n",
    "    #    new schedule \n",
    "    \n",
    "    newobject = idf1.newidfobject(\"SCHEDULE:COMPACT\")\n",
    "    newobject.Name='AllOff_WholeYear'\n",
    "    newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "    newobject.Field_1=\"Through:12/31\"\n",
    "    newobject.Field_2=\"For: AllDays\"\n",
    "    newobject.Field_3=\"Until: 24:00\"\n",
    "    newobject.Field_4=\"0\"\n",
    "    \n",
    "    print(\"New Schedule added to idf:\")\n",
    "    print(newobject)\n",
    "    \n",
    "    return(idf1)\n",
    "\n",
    "def save_idf_file(output_filename,idf_output_folder):\n",
    "    \n",
    "######Purpose of this function is to save the converted IDF object into a file in a designated folder location ##########\n",
    "######Input of the function is the output file name and the output folder path ##########################################\n",
    "######Output of this function is a printed message with the file path ###################################################    \n",
    "\n",
    "    # Save idf file to specied output folder under specified file name\n",
    "    \n",
    "    filepath=idf_output_folder+\"\\\\\"+output_filename\n",
    "\n",
    "    idf1.saveas(filepath)\n",
    "    \n",
    "    print(filepath)\n",
    "    \n",
    "    return(filepath)\n",
    "\n",
    "def ListOfFloors(idf1):\n",
    "    \n",
    "######Purpose of this function is to analyze the location of the thermal zones and identify which floor they are on#################\n",
    "######Input of the function is an idf file loaded as an IDF object using the eppy library###########################################\n",
    "######Output of this function is a count of thermal zones, as well as a list of zones with the floor level they are located in #####    \n",
    "    \n",
    "    #List all schedules in idf file\n",
    "    #Number of zones\n",
    "    idf_zones=idf1.idfobjects[\"Zone\"]\n",
    "    idf_zone_len=len(idf_zones)\n",
    "\n",
    "    #Number of floors\n",
    "\n",
    "    ##list all floors and zones\n",
    "    floor_list=[]\n",
    "    zone_list=[]\n",
    "    zonelevel_list=[]\n",
    "\n",
    "    floor_list=[zone[\"Z_Origin\"] for zone in idf_zones]\n",
    "    zone_list=[zone[\"Name\"] for zone in idf_zones]\n",
    "\n",
    "    zonelevel_list=pd.DataFrame({'Zone':zone_list,'Z_Origin':floor_list})\n",
    "\n",
    "    ##find unique z value in zone location\n",
    "    floor_list=pd.DataFrame(set(floor_list),columns=[\"Z_Origin\"])\n",
    "\n",
    "    ##order floor levels \n",
    "    floor_list_pos=floor_list[floor_list['Z_Origin']>=0].sort_values(by=['Z_Origin']).reset_index(drop=True)\n",
    "    floor_list_neg=floor_list[floor_list['Z_Origin']<0].sort_values(by=['Z_Origin'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "    floor_list_pos['floor_tag']=floor_list_pos.index\n",
    "    floor_list_neg['floor_tag']=(floor_list_neg.index+1)*-1\n",
    "\n",
    "    floor_list = floor_list_neg.append(floor_list_pos)\n",
    "    floor_list = floor_list.sort_values(by=['floor_tag'])\n",
    "\n",
    "    #Add level tag to zone\n",
    "    zonelevel_list=zonelevel_list.merge(floor_list, how='left', on='Z_Origin')\n",
    "    \n",
    "    print(\"There are {} zones in this model and {} floors\".format(idf_zone_len,len(floor_list['floor_tag'].unique())))\n",
    "\n",
    "\n",
    "    return(zonelevel_list)\n",
    "\n",
    "\n",
    "def ListOfZoneOccupantSchedule(idf1):\n",
    "    \n",
    "######Purpose of this function is to analyze the occupancy of each thermal zone#############################################\n",
    "######Input of the function is an idf file loaded as an IDF object using the eppy library###################################\n",
    "######Output of this function is a list of thermal zones with the corresponding occupancy and activity level schedules ##### \n",
    "    \n",
    "    #Occupancy Component\n",
    "    idf_people=idf1.idfobjects[\"People\"]\n",
    "\n",
    "    ##list all zones and associated schedules\n",
    "    schedule_list=[]\n",
    "    zone_list=[]\n",
    "\n",
    "    occ_sch_list=[people[\"Number_of_People_Schedule_Name\"] for people in idf_people]\n",
    "    occ_act_sch_list=[people[\"Activity_Level_Schedule_Name\"] for people in idf_people]\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        zone_list=[people[\"Zone_or_ZoneList_Name\"] for people in idf_people]\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        zone_list=[people[\"Zone_or_ZoneList_or_Space_or_SpaceList_Name\"] for people in idf_people]\n",
    "\n",
    "    zone_occ_list=pd.DataFrame({'Zone':zone_list,'Occupancy Schedule':occ_sch_list, 'Activity Schedule':occ_act_sch_list})\n",
    "\n",
    "\n",
    "    return(zone_occ_list)\n",
    " \n",
    "    \n",
    "def NewOutput_Hourly(idf1, variable):\n",
    "    \n",
    "######Purpose of this function is to add new hourly output to the simulation model##########\n",
    "######Input of the function is an idf file loaded as an IDF object using the eppy library###\n",
    "######Output of this function is the idf object with new hourly output added ##################     \n",
    "\n",
    "    newobject = idf1.newidfobject(\"OUTPUT:VARIABLE\")\n",
    "    newobject.Key_Value=\"*\"\n",
    "    newobject.Variable_Name=variable\n",
    "    newobject.Reporting_Frequency='Hourly'\n",
    "    return idf1\n",
    "\n",
    "\n",
    "\n",
    "def Analysis_Summary (idf_output_df_original,Selected_Scenario):\n",
    "    \n",
    "######Purpose of this function is to create summary table to identify zones that require additional analysis################################\n",
    "######Input of the function is the cleaned energyplus simulation output ####################################################################\n",
    "######Output of this function is a table with selected thermal zones with the worst overheating/underheating conditions based on db temp ###\n",
    "\n",
    "    Indicator='Zone Air Temperature [C](Hourly)'\n",
    "\n",
    "    Results_df=idf_output_df_original\n",
    "    Results_df_Selected=Results_df[Results_df[\"FileName\"]==Selected_Scenario]\n",
    "    Results_df_Selected=Results_df_Selected.filter(like=Indicator, axis=1)\n",
    "    Results_df_Summary=Results_df_Selected.describe().T\n",
    "\n",
    "    # Analyze Results \n",
    "\n",
    "\n",
    "    ## Results summary - Max, Min of Metric based on zone\n",
    "    Results_df_Summary=Results_df_Summary.reset_index()\n",
    "    Results_df_Summary=Results_df_Summary.rename(columns={Results_df_Summary.columns[0]: 'Zone:Metric'})\n",
    "    Results_df_Summary[['Zone','Metric']] = Results_df_Summary['Zone:Metric'].str.split(':',expand=True)\n",
    "    Results_df_Summary=Results_df_Summary.set_index('Zone:Metric')\n",
    "\n",
    "    ## Degree Hour -  Heating/Cooling Degree Hour\n",
    "    upper_temp=31\n",
    "    lower_temp=15\n",
    "\n",
    "    Cooling_Critical_Temp=upper_temp\n",
    "    Heating_Crtitical_Temp=lower_temp\n",
    "\n",
    "    HDH=lower_temp-Results_df_Selected\n",
    "    HDH[HDH<0]=0\n",
    "    Results_df_Summary['HDH']=HDH.sum()\n",
    "\n",
    "    CDH=Results_df_Selected-upper_temp\n",
    "    CDH[CDH<0]=0\n",
    "    Results_df_Summary['CDH']=CDH.sum()\n",
    "\n",
    "    Results_df_Summary\n",
    "\n",
    "    #Zone Selection\n",
    "\n",
    "    Analysis_Zone = pd.DataFrame()\n",
    "    Analysis_Zone_Num=5\n",
    "\n",
    "\n",
    "    ## Based on Max Indicator\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='max',ascending=False).reset_index()\n",
    "    Analysis_Zone['Max_Indicator']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "\n",
    "    ## Based on Min Indicator\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='min',ascending=False).reset_index()\n",
    "    Analysis_Zone['Min_Indicator']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "\n",
    "    ## Based on Max Heating Degree Hour\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='HDH',ascending=False).reset_index()\n",
    "    Analysis_Zone['HDH']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "    ## Based on Min Heating Degree Hour\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='CDH',ascending=False).reset_index()\n",
    "    Analysis_Zone['CDH']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "    Analysis_Zone=Analysis_Zone.melt()\n",
    "\n",
    "    Analysis_Zone=Analysis_Zone.rename(columns={\"value\": \"Zone\", \"variable\": \"Metric of Interst\"})\n",
    "\n",
    "\n",
    "    Analysis_Zone_Summary=Analysis_Zone.groupby('Zone')['Metric of Interst'].apply(','.join).reset_index()\n",
    "\n",
    "\n",
    "    Analysis_Zone_Summary=Analysis_Zone_Summary.merge(Results_df_Summary_Select, how='left', on='Zone')\n",
    "\n",
    "    return Analysis_Zone_Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f053578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function to read in a compact schedule from idf file and produces a dataframe that extracts fields needed to create datetime object \n",
    "\n",
    "def ScheduleConvertCompact(Schedule_Name,Type):\n",
    "    \n",
    "######Purpose of this function is to convert compact schedules#################################################################################################\n",
    "######Input of the function is the schedule name and the type of schedule #####################################################################################\n",
    "######Output of this function is the schedule object being added to the idf object. As part of the function, the schedule object is added to the idf object ###    \n",
    "\n",
    "    global Schedule\n",
    "    global dict_items\n",
    "    \n",
    "    \n",
    "    Sch_Type=Type\n",
    "    if Sch_Type == \"Occupancy\":\n",
    "        Sch_Level=\"1\"\n",
    "        \n",
    "    elif Sch_Type == \"Equipment\":\n",
    "        Sch_Level=\"0\"\n",
    "    \n",
    "    elif Sch_Type == \"Zone Control\":\n",
    "        Sch_Level=\"0\"\n",
    "        \n",
    "    elif Sch_Type == \"Heating_Setpoint\":\n",
    "        Sch_Level=\"-85\"\n",
    "        \n",
    "    elif Sch_Type == \"Cooling_Setpoint\":\n",
    "        Sch_Level=\"85\"\n",
    "    \n",
    "    else:\n",
    "        print(\"Schedule Type Not Recognized, level for disruptive event is set to 0.\")\n",
    "        Sch_Level=\"0\"\n",
    "    \n",
    "    \n",
    "    Schedule_Compact=idf1.idfobjects['SCHEDULE:COMPACT']\n",
    "    \n",
    "    \n",
    "    for x in Schedule_Compact:\n",
    "        if x.Name == Schedule_Name:\n",
    "        #copies values from schedule to a list \n",
    "            dict_items=list(x.values())\n",
    "    \n",
    "    #creates data frame from the list with schedule values\n",
    "    Sch_df=pd.DataFrame(dict_items[0], columns=['config'])\n",
    "   \n",
    "    #set schedule name and obtain schedule type\n",
    "    Sch_Header_Name=Sch_df.iloc[1]['config']\n",
    "    Sch_Header_Name=Sch_Header_Name+\"_TRmod\"\n",
    "    Sch_Header_Type=Sch_df.iloc[0]['config']\n",
    "    \n",
    "    #Remove duplicated TRmod schedules\n",
    "    for x in Schedule_Compact:\n",
    "        if x.Name == Sch_Header_Name:\n",
    "            idf1.removeidfobject(x)\n",
    "    \n",
    "    #Extracts schedule lines that contains keywords related to schedule construction \"Through\", \"Until\", \"For\" \n",
    "    #to label them in corresponding tag columns \n",
    "    Sch_df.loc[Sch_df['config'].str.contains(\"Through\", case=False),'Date_Tag']=Sch_df['config']\n",
    "    Sch_df.loc[Sch_df['config'].str.contains(\"Until\", case=False),'Time_Tag']=Sch_df['config']\n",
    "    Sch_df.loc[Sch_df['config'].str.contains(\"For\", case=False),'Day_Tag']=Sch_df['config']\n",
    "    \n",
    "    #forward fill the tags\n",
    "    Sch_df.loc[:,'Time_Tag']=Sch_df.loc[:,'Time_Tag'].ffill()\n",
    "    Sch_df.loc[:,'Date_Tag']=Sch_df.loc[:,'Date_Tag'].ffill()\n",
    "    Sch_df.loc[:,'Day_Tag']=Sch_df.loc[:,'Day_Tag'].ffill()\n",
    "    \n",
    "    #Remove rows that are not needed for the new schedule\n",
    "    Sch_df=Sch_df[Sch_df['config'].str.contains(\"For\")==False]\n",
    "    Sch_df=Sch_df[Sch_df['config'].str.contains(\"Through\")==False]\n",
    "    Sch_df=Sch_df[Sch_df['Date_Tag'].notnull()]\n",
    "    Sch_df=Sch_df[Sch_df['Time_Tag'].notnull()]\n",
    "\n",
    "    #Extract elements that are needed to assemble date time object\n",
    "    Sch_df['Date']=Sch_df['Date_Tag'].str.lower().str.extract(r'through[:]?\\s*?(\\d{1,2}/\\d{1,2})')\n",
    "    Sch_df['Time']=Sch_df['Time_Tag'].str.lower().str.extract(r'until[:]?\\s*?(\\d{1,2}:\\d{2})')\n",
    "    Sch_df[['Hour','Minute']]=Sch_df['Time'].str.split(':', expand=True)\n",
    "    \n",
    "    #need to replace hour 24 to 0\n",
    "    Sch_df['Hour_Adjusted']=Sch_df['Hour'].replace('24','0')\n",
    "    Sch_df[['Month','Day']]=Sch_df['Date'].str.split('/', expand=True).astype(int)\n",
    "    \n",
    "    #assemble date time object\n",
    "    #print(Sch_df)\n",
    "    \n",
    "    Sch_df[\"DateTime\"]= pd.to_datetime(str(RunPeriod_Year)+\"-\"+Sch_df[\"Month\"].astype(str)+\"-\"+Sch_df[\"Day\"].astype(str)+' '+ Sch_df[\"Hour_Adjusted\"].astype(str)+\":\"+Sch_df[\"Minute\"].astype(str),format='%Y-%m-%d %H:%M')\n",
    "    \n",
    "    #for hour 0, need to add a day as part of hour adjustment\n",
    "    Sch_df.loc[Sch_df['Hour_Adjusted'] == '0', 'DateTime'] = Sch_df['DateTime']+ datetime.timedelta(days=1)\n",
    "    \n",
    "    #extract rows before event start date/time\n",
    "    Sch_Before_EventStart=Sch_df.loc[Sch_df['DateTime']<Event_Start_Datetime]\n",
    "    #extract rows after event start date/time\n",
    "    Sch_After_EventStart=Sch_df.loc[Sch_df['DateTime']>=Event_Start_Datetime]\n",
    "    #extract rows after event end date/time\n",
    "    Sch_After_EventEnd=Sch_df.loc[Sch_df['DateTime']>=Event_End_Datetime]\n",
    "    #extract rows where the through date/time happens between event start and end date\n",
    "    Sch_Between_Event=Sch_After_EventStart[~Sch_After_EventStart.isin(Sch_After_EventEnd)].dropna()\n",
    "    \n",
    "    #For rows that are applicable to before event start date and after event end date, no change is needed\n",
    "    ##Adding headerlines for those lines\n",
    "    \n",
    "    Sch_DayBeforeEventStarts_DateTags=Sch_Before_EventStart['Date_Tag'].unique()\n",
    "\n",
    "    BeforeEvent_ModConfig_loop=pd.DataFrame([])\n",
    "    BeforeEvent_df_combined=pd.DataFrame([])\n",
    "\n",
    "    for datetag in Sch_DayBeforeEventStarts_DateTags:\n",
    "\n",
    "        BeforeEvent_Line1=datetag\n",
    "        Sch_Before_EventStart_SelectedDayTag=Sch_Before_EventStart.loc[Sch_Before_EventStart['Date_Tag']==datetag]\n",
    "        Sch_DayBeforeEventStarts_DayTags=Sch_Before_EventStart_SelectedDayTag['Day_Tag'].unique()\n",
    "        BeforeEvent_ModConfig_loop_Day=pd.DataFrame([])\n",
    "\n",
    "        for daytag in Sch_DayBeforeEventStarts_DayTags:\n",
    "\n",
    "            BeforeEvent_Line2=daytag\n",
    "            BeforeEvent_df_setup={'config':[BeforeEvent_Line2]}\n",
    "            BeforeEvent_df=pd.DataFrame(BeforeEvent_df_setup)\n",
    "            BeforeEvent_ModConfig_DayTagLines=Sch_Before_EventStart_SelectedDayTag.loc[Sch_Before_EventStart_SelectedDayTag['Day_Tag'].str.contains(daytag, case=False,na=False)]\n",
    "            BeforeEvent_ModConfig_loop_Day=BeforeEvent_ModConfig_loop_Day.append([BeforeEvent_df,BeforeEvent_ModConfig_DayTagLines])\n",
    "        \n",
    "        BeforeEvent_df_date_setup={'config':[BeforeEvent_Line1]}\n",
    "        BeforeEvent_df_date=pd.DataFrame(BeforeEvent_df_date_setup)\n",
    "        BeforeEvent_ModConfig_loop=BeforeEvent_ModConfig_loop.append([BeforeEvent_df_date,BeforeEvent_ModConfig_loop_Day])\n",
    "    \n",
    "    BeforeEvent_df_combined=BeforeEvent_ModConfig_loop\n",
    "    \n",
    "    ####After event ends    \n",
    "\n",
    "    Sch_DayAfterEventEnds_DateTags=Sch_After_EventEnd['Date_Tag'].unique()\n",
    "    \n",
    "    AfterEvent_ModConfig_loop=pd.DataFrame([])\n",
    "    AfterEvent_df_combined=pd.DataFrame([])\n",
    "\n",
    "    for datetag in Sch_DayAfterEventEnds_DateTags:\n",
    "        \n",
    "        AfterEvent_Line1=datetag\n",
    "        Sch_After_EventEnd_SelectedDayTag=Sch_After_EventEnd.loc[Sch_After_EventEnd['Date_Tag']==datetag]\n",
    "        Sch_DayAfterEventEnds_DayTags=Sch_After_EventEnd_SelectedDayTag['Day_Tag'].unique()\n",
    "        AfterEvent_ModConfig_loop_Day=pd.DataFrame([])                                                           \n",
    "                                                                   \n",
    "        \n",
    "        for daytag in Sch_DayAfterEventEnds_DayTags:\n",
    "\n",
    "            AfterEvent_Line2=daytag\n",
    "            AfterEvent_df_setup={'config':[AfterEvent_Line2]}\n",
    "            AfterEvent_df=pd.DataFrame(AfterEvent_df_setup)\n",
    "            AfterEvent_ModConfig_DayTagLines=Sch_After_EventEnd_SelectedDayTag.loc[Sch_After_EventEnd_SelectedDayTag['Day_Tag'].str.contains(daytag, case=False,na=False)]\n",
    "            AfterEvent_ModConfig_loop_Day=AfterEvent_ModConfig_loop_Day.append([AfterEvent_df,AfterEvent_ModConfig_DayTagLines])\n",
    "            \n",
    "        AfterEvent_df_date_setup={'config':[AfterEvent_Line1]}\n",
    "        AfterEvent_df_date=pd.DataFrame(AfterEvent_df_date_setup)\n",
    "        AfterEvent_ModConfig_loop=AfterEvent_ModConfig_loop.append([AfterEvent_df_date,AfterEvent_ModConfig_loop_Day])\n",
    "\n",
    "    AfterEvent_df_combined=AfterEvent_ModConfig_loop\n",
    "\n",
    "    \n",
    "    #Assemble Logic for event\n",
    "    ## Logic for day before Event Start\n",
    "    ### Search for rows that belongs to the earliest schedule through date that is after event start\n",
    "    Sch_DayBefore_Event=Sch_After_EventStart.loc[Sch_After_EventStart['Date']==min(Sch_After_EventStart['Date'])]\n",
    "\n",
    "    #creating logic to assemble date time(in idf format) for the days before disruptive event starts(where schedule thru date is after event start time)\n",
    "    Event_DayBeforeStart=Event_Start_Datetime.date()+datetime.timedelta(days=-1)\n",
    "    Event_DayBeforeStart_idfformat=\"{}/{:02d}\".format(Event_DayBeforeStart.month,Event_DayBeforeStart.day)\n",
    "    \n",
    "    Sch_DayBefore_Event_DayTags=Sch_DayBefore_Event['Day_Tag'].unique()\n",
    "    i=0\n",
    "    Sch_DayBefore_Event_ModConfig_loop=pd.DataFrame([])\n",
    "    for daytags in Sch_DayBefore_Event_DayTags:\n",
    "        \n",
    "        PreDistruptive_Line2=Sch_DayBefore_Event['Day_Tag'].unique()[i]\n",
    "        Disruptive_df_DayBefore_setup={'config':[PreDistruptive_Line2]}\n",
    "        Disruptive_df_DayBefore=pd.DataFrame(Disruptive_df_DayBefore_setup)\n",
    "        Sch_DayBefore_Event_ModConfig_DayTagLines=Sch_DayBefore_Event[Sch_DayBefore_Event['Day_Tag'].str.contains(daytags)]\n",
    "        Sch_DayBefore_Event_ModConfig_loop=pd.concat([Sch_DayBefore_Event_ModConfig_loop,Disruptive_df_DayBefore,Sch_DayBefore_Event_ModConfig_DayTagLines])\n",
    "        i=i+1\n",
    "        \n",
    "    PreDistruptive_Line1=\"Through: {}\".format(Event_DayBeforeStart_idfformat)\n",
    "    Disruptive_df_DayBefore_setup_line1={'config':[PreDistruptive_Line1]}\n",
    "    Disruptive_df_DayBefore_line1=pd.DataFrame(Disruptive_df_DayBefore_setup_line1)\n",
    "    \n",
    "    \n",
    "    Sch_DayBefore_Event_ModConfig=pd.concat([Disruptive_df_DayBefore_line1,Sch_DayBefore_Event_ModConfig_loop])\n",
    "    \n",
    "    ## Logic for Event Start Day\n",
    "    ###First need to determine which day logic should be used (AllDay, Holiday, {Actual Weekday Days}, Weekend,Weekday or AllOtherDays)\n",
    "    ### Sch_JustBeforeEvent refers to hours right before event begins\n",
    "    \n",
    "    Sch_JustBefore_Event=Sch_DayBefore_Event\n",
    "    \n",
    "    if Sch_JustBefore_Event['Day_Tag'].str.contains('AllDays', case=False).any():\n",
    "    \n",
    "        Sch_JustBefore_Event_ModConfig_DayTagLines=Sch_DayBefore_Event[Sch_DayBefore_Event['Day_Tag'].str.contains('AllDays', case=False)]\n",
    "\n",
    "    elif (Sch_JustBefore_Event['Day_Tag'].str.contains('Holidays', case=False).any() and EventDate_Analyze_df[\"Is_Holiday\"].iloc[0] == True):\n",
    "    \n",
    "        Sch_JustBefore_Event_ModConfig_DayTagLines=Sch_DayBefore_Event[Sch_DayBefore_Event['Day_Tag'].str.contains('Holidays', case=False)]\n",
    "\n",
    "    elif (Sch_JustBefore_Event['Day_Tag'].str.contains('Weekend', case=False).any() and EventDate_Analyze_df[\"Is_Weekend\"].iloc[0] == True):\n",
    "    \n",
    "        Sch_JustBefore_Event_ModConfig_DayTagLines=Sch_DayBefore_Event[Sch_DayBefore_Event['Day_Tag'].str.contains('Weekend', case=False)] \n",
    "    \n",
    "    elif (Sch_JustBefore_Event['Day_Tag'].str.contains('Weekday', case=False).any() and EventDate_Analyze_df[\"Is_Weekday\"].iloc[0] == True):\n",
    "    \n",
    "        Sch_JustBefore_Event_ModConfig_DayTagLines=Sch_DayBefore_Event[Sch_DayBefore_Event['Day_Tag'].str.contains('Weekday', case=False)]\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        Sch_JustBefore_Event_ModConfig_DayTagLines=Sch_DayBefore_Event[Sch_DayBefore_Event['Day_Tag'].str.contains('AllOtherDays', case=False)]\n",
    "        \n",
    "    \n",
    "    ### Header Line for Event Start Day\n",
    "    EventStartDay_Line1=\"Through: {}\".format(Event_StartDate)\n",
    "    EventStartDay_Line2=\"For: AllDays\"\n",
    "    \n",
    "    Disruptive_df_EventStartDay_setup={'config':[EventStartDay_Line1,EventStartDay_Line2]}\n",
    "    Disruptive_df_EventStartDay=pd.DataFrame(Disruptive_df_EventStartDay_setup)\n",
    "\n",
    "    Disruptive_BeforeEvent=Sch_JustBefore_Event_ModConfig_DayTagLines[Sch_JustBefore_Event_ModConfig_DayTagLines['DateTime'].dt.time<Event_Start_Datetime.time()]\n",
    "    #remove last 2 rows - until 24:00 - the time for hour 24 will always be the lowest. \n",
    "    Disruptive_BeforeEvent=Disruptive_BeforeEvent.drop(Disruptive_BeforeEvent.tail(2).index)\n",
    "    #add logic for setting until event start time\n",
    "    Disruptive_BeforeEvent2header=\"Until: {}\".format(Event_StartTime)\n",
    "    \n",
    "    Disruptive_BeforeEvent2=Sch_JustBefore_Event_ModConfig_DayTagLines[Sch_JustBefore_Event_ModConfig_DayTagLines['DateTime'].dt.time>=Event_Start_Datetime.time()]\n",
    "    \n",
    "    ###TS####\n",
    "    Disruptive_BeforeEvent2=Sch_JustBefore_Event_ModConfig_DayTagLines[Sch_JustBefore_Event_ModConfig_DayTagLines['DateTime'].dt.time>=Event_Start_Datetime.time()]\n",
    "    if not Disruptive_BeforeEvent2.size:\n",
    "        print(\"list empty\")\n",
    "        Disruptive_BeforeEvent2=Sch_JustBefore_Event_ModConfig_DayTagLines\n",
    "    else:\n",
    "        print(\"list not empty\")\n",
    "    \n",
    "    ###TS####\n",
    "    #print(Sch_JustBefore_Event_ModConfig_DayTagLines)\n",
    "    \n",
    "    ###TS####\n",
    "\n",
    "\n",
    "\n",
    "    Disruptive_BeforeEvent2=Disruptive_BeforeEvent2.iloc[1]['config']\n",
    "    \n",
    "    \n",
    "    ### Logic for different end date/time\n",
    "    Disruptive_TimeDelta = Event_End_Datetime.date() - Event_Start_Datetime.date()\n",
    "    Disruptive_TimeDelta_Days=Disruptive_TimeDelta.days\n",
    "    \n",
    "    #print(Disruptive_TimeDelta_Days)\n",
    "    \n",
    "    if Event_End_Datetime.date() == Event_Start_Datetime.date():\n",
    "        DisruptiveMode = \"Same Day\"\n",
    "    elif Disruptive_TimeDelta_Days < 2:\n",
    "        DisruptiveMode = \"Not Same Day\"\n",
    "    else:\n",
    "        DisruptiveMode = \"Not Same Day - Multiple Days\"\n",
    "    \n",
    "    \n",
    "    print(DisruptiveMode)\n",
    "    \n",
    "    ## if event lasts more than 24 hours\n",
    "    \n",
    "    if DisruptiveMode == \"Not Same Day - Multiple Days\" or DisruptiveMode == \"Not Same Day\":\n",
    "        print(\"mode1\")\n",
    "        EventStartDay_Line3=\"Until: 24:00\"\n",
    "        EventStartDay_Line4=Sch_Level\n",
    "        \n",
    "    elif DisruptiveMode == \"Same Day\":\n",
    "        print(\"mode2\")\n",
    "        EventStartDay_Line3=\"Until: {}\".format(Event_EndTime)\n",
    "        EventStartDay_Line4=Sch_Level\n",
    "        \n",
    "    else:\n",
    "        print(\"mode3\")\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    Disruptive_df_EventStartDay_setup_2={'config':[Disruptive_BeforeEvent2header,Disruptive_BeforeEvent2,EventStartDay_Line3,EventStartDay_Line4]}\n",
    "    \n",
    "    Disruptive_df_EventStartDay_2=pd.DataFrame(Disruptive_df_EventStartDay_setup_2)\n",
    "\n",
    "    Disruptive_BeforeEvent_Combined=pd.concat([Disruptive_df_EventStartDay,Disruptive_BeforeEvent,Disruptive_df_EventStartDay_2])\n",
    "    \n",
    "    ## Logic for in between event days (day after start and day before end)\n",
    "    \n",
    "    Event_DayBeforeEnd=Event_End_Datetime.date()+datetime.timedelta(days=-1)\n",
    "    Event_DayBeforeEnd_idfformat=\"{}/{:02d}\".format(Event_DayBeforeEnd.month,Event_DayBeforeEnd.day)\n",
    "\n",
    "    EventInBetweenDay_Line1=\"Through: {}\".format(Event_DayBeforeEnd_idfformat)\n",
    "    EventInBetweenDay_Line2=\"For: AllDays\" \n",
    "    EventInBetweenDay_Line3=\"Until: 24:00\"\n",
    "    EventInBetweenDay_Line4=Sch_Level\n",
    "\n",
    "    \n",
    "    Disruptive_df_EventInBetweenDay_setup={'config':[EventInBetweenDay_Line1,EventInBetweenDay_Line2,EventInBetweenDay_Line3,EventInBetweenDay_Line4]}\n",
    "    \n",
    "    Disruptive_df_EventInBetweenDay=pd.DataFrame(Disruptive_df_EventInBetweenDay_setup)\n",
    "    \n",
    "    ## Logic for Event End Day\n",
    "\n",
    "    EventEndDay_Line1=\"Through: {}\".format(Event_EndDate)\n",
    "    EventEndDay_Line2=\"For: AllDays\"\n",
    "    EventEndDay_Line3=\"Until: {}\".format(Event_EndTime)\n",
    "    EventEndDay_Line4=Sch_Level\n",
    "\n",
    "    Disruptive_df_EventEndDay_setup={'config':[EventEndDay_Line1,EventEndDay_Line2,EventEndDay_Line3,EventEndDay_Line4]}\n",
    "    Disruptive_df_EventEndDay=pd.DataFrame(Disruptive_df_EventEndDay_setup)\n",
    "    \n",
    "    Sch_DayAfter_Event=Sch_After_EventEnd.loc[Sch_After_EventEnd['Date']==min(Sch_After_EventEnd['Date'])]\n",
    "    Sch_JustAfter_Event=Sch_DayAfter_Event\n",
    "\n",
    "    if Sch_JustAfter_Event['Day_Tag'].str.contains('AllDays', case=False).any():\n",
    "    \n",
    "        Sch_JustAfter_Event_ModConfig_DayTagLines=Sch_DayAfter_Event[Sch_DayAfter_Event['Day_Tag'].str.contains('AllDays', case=False)]\n",
    "\n",
    "    elif (Sch_JustAfter_Event['Day_Tag'].str.contains('Holidays', case=False).any() and EventDate_Analyze_df[\"Is_Holiday\"].iloc[0] == True):\n",
    "    \n",
    "        Sch_JustAfter_Event_ModConfig_DayTagLines=Sch_DayAfter_Event[Sch_DayAfter_Event['Day_Tag'].str.contains('Holidays', case=False)]\n",
    "\n",
    "    elif (Sch_JustAfter_Event['Day_Tag'].str.contains('Weekend', case=False).any() and EventDate_Analyze_df[\"Is_Weekend\"].iloc[0] == True):\n",
    "    \n",
    "        Sch_JustAfter_Event_ModConfig_DayTagLines=Sch_DayAfter_Event[Sch_DayAfter_Event['Day_Tag'].str.contains('Weekend', case=False)] \n",
    "    \n",
    "    elif (Sch_JustAfter_Event['Day_Tag'].str.contains('Weekday', case=False).any() and EventDate_Analyze_df[\"Is_Weekday\"].iloc[0] == True):\n",
    "    \n",
    "        Sch_JustAfter_Event_ModConfig_DayTagLines=Sch_DayAfter_Event[Sch_DayAfter_Event['Day_Tag'].str.contains('Weekday', case=False)]\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        Sch_JustAfter_Event_ModConfig_DayTagLines=Sch_DayAfter_Event[Sch_DayAfter_Event['Day_Tag'].str.contains('AllOtherDays', case=False)]\n",
    "\n",
    "    \n",
    "    \n",
    "    Disruptive_Sch_JustAfter_Event_1=Sch_JustAfter_Event_ModConfig_DayTagLines[Sch_JustAfter_Event_ModConfig_DayTagLines['DateTime'].dt.time>Event_End_Datetime.time()]\n",
    "    Disruptive_Sch_JustAfter_Event_2=Sch_JustAfter_Event_ModConfig_DayTagLines[Sch_JustAfter_Event_ModConfig_DayTagLines['Hour']=='24']\n",
    "\n",
    "    Disruptive_Sch_JustAfter_Event=pd.concat([Disruptive_Sch_JustAfter_Event_1,Disruptive_Sch_JustAfter_Event_2])\n",
    "    \n",
    "    if DisruptiveMode == \"Same Day\":\n",
    "        Disruptive_AfterEvent_Combined=pd.concat([Disruptive_Sch_JustAfter_Event])\n",
    "        \n",
    "    else:    \n",
    "        Disruptive_AfterEvent_Combined=pd.concat([Disruptive_df_EventEndDay,Disruptive_Sch_JustAfter_Event])\n",
    "        display(HTML(Disruptive_AfterEvent_Combined.to_html()))\n",
    "    \n",
    "    \n",
    "    if DisruptiveMode == \"Not Same Day - Multiple Days\":\n",
    "    \n",
    "        Sch_Mod_Combined=pd.concat([BeforeEvent_df_combined,Sch_DayBefore_Event_ModConfig,Disruptive_BeforeEvent_Combined,Disruptive_df_EventInBetweenDay,Disruptive_AfterEvent_Combined,AfterEvent_df_combined])\n",
    "        #print(AfterEvent_df_combined)\n",
    "        display(HTML(AfterEvent_df_combined.to_html()))\n",
    "    \n",
    "    elif DisruptiveMode == \"Not Same Day\":\n",
    "        Sch_Mod_Combined=pd.concat([BeforeEvent_df_combined,Sch_DayBefore_Event_ModConfig,Disruptive_BeforeEvent_Combined,Disruptive_AfterEvent_Combined,AfterEvent_df_combined])\n",
    "        \n",
    "        \n",
    "    elif DisruptiveMode == \"Same Day\":\n",
    "    \n",
    "        Sch_Mod_Combined=pd.concat([BeforeEvent_df_combined,Sch_DayBefore_Event_ModConfig,Disruptive_BeforeEvent_Combined,Disruptive_AfterEvent_Combined,AfterEvent_df_combined])\n",
    "    \n",
    "    Sch_Mod_Combined.reset_index(inplace=True)\n",
    "    Sch_Mod_Combined[\"Field_Num\"]=Sch_Mod_Combined.index+1\n",
    "    Sch_Mod_Combined[\"Field\"]=\"Field_\"+Sch_Mod_Combined[\"Field_Num\"].astype(str)\n",
    "    \n",
    "    newobject = idf1.newidfobject(\"Schedule:Compact\")\n",
    "    newobject.Name= Sch_Header_Name\n",
    "    \n",
    "    if Sch_Type == \"Zone Control\":\n",
    "        newobject.Schedule_Type_Limits_Name=\"Control Type\"\n",
    "        \n",
    "    elif (Sch_Type == \"Heating_Setpoint\") or (Sch_Type == \"Cooling_Setpoint\"):\n",
    "        newobject.Schedule_Type_Limits_Name=\"Temperature\"\n",
    "        \n",
    "    else:\n",
    "        newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "    \n",
    "    for index, row in Sch_Mod_Combined.iterrows():\n",
    "        field_name=row['Field']\n",
    "        newobject[field_name]=row[\"config\"]\n",
    "    \n",
    "    \n",
    "    print(\"New schedule {} added\".format(Sch_Header_Name))\n",
    "    print(newobject)\n",
    "    \n",
    "    return (newobject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28777ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScheduleConvertYearly(Schedule_Name,Type):\n",
    "\n",
    "######Purpose of this function is to convert yearly schedules#################################################################################################\n",
    "######Input of the function is the schedule name and the type of schedule #####################################################################################\n",
    "######Output of this function is the schedule object being added to the idf object. As part of the function, the schedule object is added to the idf object ### \n",
    "    \n",
    "    global Schedule\n",
    "    global dict_items\n",
    "    \n",
    "    Sch_Type=Type\n",
    "    if Sch_Type == \"Occupancy\":\n",
    "        Sch_Level=\"1\"\n",
    "        \n",
    "    elif Sch_Type == \"Equipment\":\n",
    "        Sch_Level=\"0\"\n",
    "    \n",
    "    elif Sch_Type == \"Zone Control\":\n",
    "        Sch_Level=\"0\"\n",
    "        \n",
    "    elif Sch_Type == \"Zone Control\":\n",
    "        Sch_Level=\"0\"\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(\"Schedule Type Not Recognized, level for disruptive event is set to 0.\")\n",
    "        Sch_Level=\"0\"\n",
    "    Schedule_Year=idf1.idfobjects['SCHEDULE:YEAR']\n",
    "\n",
    "    for x in Schedule_Year:\n",
    "    ##* revised after test 1\n",
    "        if x.Name == Schedule_Name:\n",
    "            dict_items=list(x.values())\n",
    "            dict_name=list(x.objls)\n",
    "\n",
    "    #creates dataframe from the list with schedule values\n",
    "    Sch_df=pd.DataFrame(dict_items[0], columns=['config'])\n",
    "    Sch_df['Key']=dict_name[:len(Sch_df['config'])]\n",
    "\n",
    "    #set schedule name and obtain schedule type\n",
    "    Sch_Header_Name=Sch_df.iloc[1]['config']\n",
    "    Sch_Header_Type=Sch_df.iloc[0]['config']\n",
    "    Sch_Header_Limit_Type=Sch_df.iloc[2]['config']\n",
    "\n",
    "    ### Logic for different end date/time\n",
    "    Disruptive_TimeDelta = Event_End_Datetime.date() - Event_Start_Datetime.date()\n",
    "    Disruptive_TimeDelta_Days=Disruptive_TimeDelta.days\n",
    "\n",
    "    if Event_End_Datetime.date() == Event_Start_Datetime.date():\n",
    "        DisruptiveMode = \"Same Day\"\n",
    "    elif Disruptive_TimeDelta_Days < 2:\n",
    "        DisruptiveMode = \"Not Same Day\"\n",
    "    else:\n",
    "        DisruptiveMode = \"Not Same Day - Multiple Days\"\n",
    "\n",
    "    print(\"DisruptiveMode is {}\".format(DisruptiveMode))    \n",
    "\n",
    "    ## if event lasts more than 24 hours\n",
    "\n",
    "    if DisruptiveMode == \"Not Same Day - Multiple Days\" or DisruptiveMode == \"Not Same Day\":\n",
    "        EventStartDay_Line3=\"Until: 24:00\"\n",
    "        EventStartDay_Line4=Sch_Level\n",
    "\n",
    "    elif DisruptiveMode == \"Same Day\":\n",
    "        EventStartDay_Line3=\"Until: {}\".format(Event_EndTime)\n",
    "        EventStartDay_Line4=Sch_Level\n",
    "\n",
    "\n",
    "    Sch_df=Sch_df.iloc[3:]\n",
    "    Sch_df.loc[Sch_df['Key'].str.contains(\"ScheduleWeek_Name\", case=False),'WeekSchName']=Sch_df['config']\n",
    "    Sch_df.loc[:,'WeekSchName']=Sch_df.loc[:,'WeekSchName'].ffill()\n",
    "    Sch_df['Key']=Sch_df['Key'].str.rsplit('_', n=1).str[0]\n",
    "    Sch_df=Sch_df.pivot(index='WeekSchName',columns='Key', values='config').reset_index(drop=True)\n",
    "\n",
    "    cols=['ScheduleWeek_Name','Start_Month','Start_Day','End_Month','End_Day']\n",
    "\n",
    "    Sch_df=Sch_df[cols]\n",
    "    Sch_df['Start_Date']=pd.to_datetime(str(RunPeriod_Year)+\"-\"+Sch_df[\"Start_Month\"].astype(str)+\"-\"+Sch_df[\"Start_Day\"].astype(str)+\" 00:00\",format='%Y-%m-%d %H:%M')\n",
    "    Sch_df['End_Date']=pd.to_datetime(str(RunPeriod_Year)+\"-\"+Sch_df[\"End_Month\"].astype(str)+\"-\"+Sch_df[\"End_Day\"].astype(str)+\" 00:00\",format='%Y-%m-%d %H:%M')\n",
    "    Sch_df['End_Date']=Sch_df['End_Date']+ datetime.timedelta(days=1)\n",
    "\n",
    "\n",
    "    StartSchName=Sch_df.loc[(Sch_df['Start_Date'] <= Event_Start_Datetime) & (Sch_df['End_Date'] > Event_Start_Datetime)]['ScheduleWeek_Name'][0]\n",
    "    EndSchName=Sch_df.loc[(Sch_df['Start_Date'] <= Event_End_Datetime) & (Sch_df['End_Date'] > Event_End_Datetime)]['ScheduleWeek_Name'][0]\n",
    "\n",
    "    # Get appropriate weekly schedule\n",
    "\n",
    "    Week_Sch_Daily_idf=idf1.idfobjects['SCHEDULE:WEEK:DAILY']\n",
    "    Week_Sch_Compact_idf=idf1.idfobjects['SCHEDULE:WEEK:COMPACT']\n",
    "\n",
    "    for i in Week_Sch_Daily_idf:    \n",
    "        if i.Name == StartSchName:\n",
    "            StartSch=i\n",
    "            StartSchType=\"Week:Daily\"\n",
    "\n",
    "        if i.Name == EndSchName:\n",
    "            EndSch=i\n",
    "            EndSchType=\"Week:Daily\"\n",
    "\n",
    "    for i in Week_Sch_Compact_idf:    \n",
    "        if i.Name == StartSchName:\n",
    "            StartSch=i\n",
    "            StartSchType=\"Week:Compact\"\n",
    "\n",
    "        if i.Name == EndSchName:\n",
    "            EndSch=i\n",
    "            EndSchType=\"Week:Compact\"        \n",
    "    ############################################Find Day Schedule from Week Schedule##################################################        \n",
    "\n",
    "    if (StartSchType == \"Week:Daily\" and EndSchType == \"Week:Daily\"):     \n",
    "        print('Both start and end weekly schedule type are Week:Daily schedules.')\n",
    "\n",
    "        # Find Day Schedule for Start Day       \n",
    "        StartSchWeek_daily_df=pd.DataFrame(list(StartSch.values())[0], columns=['config'])\n",
    "        StartSchWeek_daily_df['Key']=list(StartSch.values())[1]    \n",
    "        StartSchWeek=StartSchWeek_daily_df.copy()\n",
    "        StartSchWeek=StartSchWeek.iloc[2:]\n",
    "\n",
    "        StartSchWeek['Day']=\"Is_\"+StartSchWeek['Key'].str.split('_', n=1).str[0]\n",
    "\n",
    "        EventDate_Analyze_Start_Yearly=EventDate_Analyze_df[:1]\n",
    "\n",
    "\n",
    "        if EventDate_Analyze_Start_Yearly['Is_Holiday'][0] != False:\n",
    "\n",
    "            if EventDate_Analyze_Start_Yearly['Is_Holiday'][0] == 'CustomDay1':\n",
    "                StartSchWeek_DaySchName=StartSchWeek.loc[StartSchWeek['Key']==\"CustomDay1_ScheduleDay_Name\"]\n",
    "                StartSchWeek_DaySchName=StartSchWeek_DaySchName['config'].iloc[0]\n",
    "\n",
    "            elif EventDate_Analyze_Start_Yearly['Is_Holiday'][0] == 'CustomDay2':\n",
    "\n",
    "                StartSchWeek_DaySchName=StartSchWeek.loc[StartSchWeek['Key']==\"CustomDay2_ScheduleDay_Name\"]\n",
    "                StartSchWeek_DaySchName=StartSchWeek_DaySchName['config'].iloc[0]\n",
    "\n",
    "            else:\n",
    "                StartSchWeek_DaySchName=StartSchWeek.loc[StartSchWeek['Key']==\"Holiday_ScheduleDay_Name\"]\n",
    "                StartSchWeek_DaySchName=StartSchWeek_DaySchName['config'].iloc[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('not holiday')\n",
    "            EventDate_Analyze_Start_Yearly=EventDate_Analyze_Start_Yearly.melt()[3:]\n",
    "            EventDate_Analyze_Start_Yearly=EventDate_Analyze_Start_Yearly.loc[EventDate_Analyze_Start_Yearly['value'] == True]\n",
    "            EventDate_Analyze_Start_Yearly_Day=EventDate_Analyze_Start_Yearly['variable'].iloc[0]\n",
    "\n",
    "            StartSchWeek=StartSchWeek.loc[StartSchWeek['Day']==EventDate_Analyze_Start_Yearly_Day]\n",
    "            StartSchWeek_DaySchName=StartSchWeek['config'].iloc[0]\n",
    "\n",
    "        # Find Day Schedule for End Day \n",
    "\n",
    "        EndSchWeek_df=pd.DataFrame(list(EndSch.values())[0], columns=['config'])\n",
    "\n",
    "        EndSchWeek_df['Key']=list(EndSch.values())[1]    \n",
    "\n",
    "        EndSchWeek=EndSchWeek_df.copy()\n",
    "        EndSchWeek=EndSchWeek[2:]\n",
    "        EndSchWeek['Day']=\"Is_\"+EndSchWeek['Key'].str.split('_', n=1).str[0]\n",
    "\n",
    "        EventDate_Analyze_End_Yearly=EventDate_Analyze_df[1:].reset_index(drop=True)\n",
    "\n",
    "        if EventDate_Analyze_End_Yearly['Is_Holiday'][0] != False:\n",
    "\n",
    "            if EventDate_Analyze_End_Yearly['Is_Holiday'][0] == 'CustomDay1':\n",
    "                EndSchWeek_DaySchName=EndSchWeek.loc[EndSchWeek['Key']==\"CustomDay1_ScheduleDay_Name\"]\n",
    "                EndSchWeek_DaySchName=EndSchWeek_DaySchName['config'].iloc[0]\n",
    "\n",
    "            elif EventDate_Analyze_End_Yearly['Is_Holiday'][0] == 'CustomDay2':\n",
    "\n",
    "                EndSchWeek_DaySchName=EndSchWeek.loc[EndSchWeek['Key']==\"CustomDay2_ScheduleDay_Name\"]\n",
    "                EndSchWeek_DaySchName=EndSchWeek_DaySchName['config'].iloc[0]\n",
    "\n",
    "            else:\n",
    "                EndSchWeek_DaySchName=EndSchWeek.loc[EndSchWeek['Key']==\"Holiday_ScheduleDay_Name\"]\n",
    "                EndSchWeek_DaySchName=EndSchWeek_DaySchName['config'].iloc[0]            \n",
    "        else:\n",
    "\n",
    "            EventDate_Analyze_End_Yearly=EventDate_Analyze_End_Yearly.melt()[3:]\n",
    "            EventDate_Analyze_End_Yearly=EventDate_Analyze_End_Yearly.loc[EventDate_Analyze_End_Yearly['value'] == True]\n",
    "            EventDate_Analyze_End_Yearly_Day=EventDate_Analyze_End_Yearly['variable'].iloc[0]\n",
    "\n",
    "            EndSchWeek=EndSchWeek.loc[EndSchWeek['Day']==EventDate_Analyze_End_Yearly_Day]\n",
    "\n",
    "\n",
    "            EndSchWeek_DaySchName=EndSchWeek['config'].iloc[0] \n",
    "\n",
    "    if (StartSchType == \"Week:Compact\" and EndSchType == \"Week:Compact\"):\n",
    "\n",
    "        print(\"placeholder for week compact\")\n",
    "\n",
    "    ############################################Get and Modify Actual Day Schedule##################################################\n",
    "\n",
    "    Day_Sch_Hourly_idf=idf1.idfobjects['SCHEDULE:Day:Hourly']\n",
    "    Day_Sch_Interval_idf=idf1.idfobjects['SCHEDULE:Day:Interval']\n",
    "    Day_Sch_List_idf=idf1.idfobjects['SCHEDULE:Day:List']\n",
    "\n",
    "    for i in Day_Sch_Hourly_idf:    \n",
    "        if i.Name == StartSchWeek_DaySchName:\n",
    "            StartSchDay=i\n",
    "            StartSchTypeDay=\"Day:Hourly\"        \n",
    "\n",
    "        if i.Name == EndSchWeek_DaySchName:\n",
    "            EndSchDay=i\n",
    "            EndSchTypeDay=\"Day:Hourly\"\n",
    "\n",
    "    for i in Day_Sch_Interval_idf:    \n",
    "        if i.Name == StartSchWeek_DaySchName:\n",
    "            StartSchDay=i\n",
    "            StartSchTypeDay=\"Day:Interval\"\n",
    "\n",
    "        if i.Name == StartSchWeek_DaySchName:\n",
    "            EndSchDay=i\n",
    "            EndSchTypeDay=\"Day:Interval\"        \n",
    "\n",
    "    for i in Day_Sch_List_idf:    \n",
    "        if i.Name == StartSchWeek_DaySchName:\n",
    "            StartSchDay=i\n",
    "            StartSchTypeDay=\"Day:List\"\n",
    "\n",
    "        if i.Name == StartSchWeek_DaySchName:\n",
    "            EndSchDay=i\n",
    "            EndSchTypeDay=\"Day:List\"  \n",
    "\n",
    "    if (StartSchTypeDay == \"Day:Hourly\" and EndSchTypeDay == \"Day:Hourly\"): \n",
    "\n",
    "        print('Day:Hourly Schedule is used, but not recommended, since partial hours are not supported. Event will begin at the end on the hour')\n",
    "\n",
    "        StartSchDay_df=pd.DataFrame(list(StartSchDay.values())[0], columns=['config'])\n",
    "        StartSchDay_df['Key']=list(StartSchDay.values())[1]\n",
    "        StartSchDay_Header=StartSchDay_df[:3]\n",
    "        StartSchDay_Hourly_df=StartSchDay_df[3:].copy()\n",
    "        StartSchDay_Hourly_df['Hour']=StartSchDay_Hourly_df['Key'].str.rsplit('_', n=1).str[1]\n",
    "\n",
    "        EndSchDay_df=pd.DataFrame(list(EndSchDay.values())[0], columns=['config'])\n",
    "        EndSchDay_df['Key']=list(EndSchDay.values())[1]\n",
    "        EndSchDay_Header=EndSchDay_df[:3]\n",
    "        EndSchDay_Hourly_df=EndSchDay_df[3:].copy()\n",
    "        EndSchDay_Hourly_df['Hour']=EndSchDay_Hourly_df['Key'].str.rsplit('_', n=1).str[1]\n",
    "\n",
    "\n",
    "        Event_Start_Hour=Event_Start_Datetime.hour\n",
    "        Event_End_Hour=Event_End_Datetime.hour\n",
    "\n",
    "        if DisruptiveMode == \"Same Day\":\n",
    "\n",
    "            print(\"Disruptive event starts and ends on the same day\")\n",
    "\n",
    "            StartSchDay_Hourly_df.loc[(StartSchDay_Hourly_df['Hour'].astype('int')>=Event_Start_Hour) & (StartSchDay_Hourly_df['Hour'].astype('int')<=Event_End_Hour), ['config']]=Sch_Level\n",
    "            Day_Sch_df=pd.concat([StartSchDay_Header,StartSchDay_Hourly_df])\n",
    "            Start_Sch_Header_Name=StartSchDay_Header.loc[1,'config']\n",
    "\n",
    "            #Remove Existing TRMod schedules if it already exists\n",
    "\n",
    "            for x in list(Day_Sch_Hourly_idf):\n",
    "                if x.Name == Start_Sch_Header_Name + \"_Day_TRMod\":\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            #Add Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Day:Hourly\")\n",
    "            newobject.Name= Start_Sch_Header_Name + \"_Day_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in Day_Sch_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]\n",
    "\n",
    "            ######################################Assemble Week Schedule#####################################\n",
    "\n",
    "            #Remove Existing TRMod schedules if it already exists \n",
    "            for x in list(Week_Sch_Compact_idf):\n",
    "                if x.Name == StartSchName + 'TRWeekSch_DisruptDay':\n",
    "                    print(\"\")\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            #create new week schedule\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=StartSchName + 'TRWeekSch_DisruptDay'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']= Start_Sch_Header_Name + \"_Day_TRMod\"\n",
    "\n",
    "\n",
    "        elif DisruptiveMode == \"Not Same Day - Multiple Days\" or DisruptiveMode == \"Not Same Day\":\n",
    "\n",
    "            print(\"Disruptive event starts and ends on separate days\")\n",
    "\n",
    "            StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['Hour'].astype('int')>=Event_Start_Hour, ['config']]=Sch_Level\n",
    "            EndSchDay_Hourly_df.loc[EndSchDay_Hourly_df['Hour'].astype('int')< Event_End_Hour, ['config']]=Sch_Level\n",
    "            InBetween_Hourly_df=StartSchDay_Hourly_df.copy()\n",
    "            InBetween_Hourly_df['config']=Sch_Level       \n",
    "\n",
    "            #Remove Existing TRMod schedules if it already exists \n",
    "            for x in list(Day_Sch_Hourly_idf):\n",
    "                if (x.Name == Start_Sch_Header_Name + \"_StartDay_TRMod\") or (x.Name == End_Sch_Header_Name + \"_EndDay_TRMod\") or (x.Name == Sch_Type + \"_InBetweenSch_TRMod\"):\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            #Add Start Day Schedule\n",
    "            newobject = idf1.newidfobject(\"Schedule:Day:Hourly\")\n",
    "            Start_Sch_Header_Name=StartSchDay_Header.loc[1,'config']\n",
    "            newobject.Name= Start_Sch_Header_Name + \"_StartDay_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in StartSchDay_Hourly_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]\n",
    "\n",
    "            #Add End Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Day:Hourly\")\n",
    "            End_Sch_Header_Name=EndSchDay_Header.loc[1,'config']\n",
    "            newobject.Name= End_Sch_Header_Name + \"_EndDay_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in EndSchDay_Hourly_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]\n",
    "\n",
    "            #Add In-between Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Day:Hourly\")\n",
    "            newobject.Name= Sch_Type + \"_InBetweenSch_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in InBetween_Hourly_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]\n",
    "\n",
    "\n",
    "            #######################################Assemble Week Schedules ##########################################    \n",
    "            #Remove Existing TRMod schedules if it already exists\n",
    "\n",
    "            for x in list(Week_Sch_Compact_idf):\n",
    "                if (x.Name == StartSchName + 'TRWeekSch_Start') or (x.Name == EndSchName + 'TRWeekSch_End') or (x.Name == Sch_Type + 'TRWeekSch_InBetween'):\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=StartSchName + 'TRWeekSch_Start'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']=Start_Sch_Header_Name + \"_StartDay_TRMod\"\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=EndSchName +'TRWeekSch_End'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']=End_Sch_Header_Name + \"_EndDay_TRMod\"\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=Sch_Type + 'TRWeekSch_InBetween'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']=Sch_Type + \"_InBetweenSch_TRMod\"\n",
    "\n",
    "\n",
    "    if (StartSchTypeDay == \"Day:Interval\" and EndSchTypeDay == \"Day:Interval\"):\n",
    "\n",
    "        print('Day:Interval Schedule is used')\n",
    "\n",
    "        StartSchDay_df=pd.DataFrame(list(StartSchDay.values())[0], columns=['config'])\n",
    "        StartSchDay_Header=StartSchDay_df[:4]\n",
    "        StartSchDay_Hourly_df=StartSchDay_df[4:].copy()\n",
    "        \n",
    "        print(StartSchDay_Hourly_df)\n",
    "        \n",
    "        ##* Modified after test 1 \n",
    "        \n",
    "        if StartSchDay_Hourly_df['config'].str.contains(\"Until\", case=False,na=False).sum() >= 1:\n",
    "        \n",
    "            StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['config'].str.contains(\"Until\", case=False,na=False),'Time_Tag']=StartSchDay_Hourly_df['config']\n",
    "        else:\n",
    "            StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['config'].str.contains(\":\", case=False,na=False),'Time_Tag']=\"Until: \"+StartSchDay_Hourly_df['config'].astype(\"string\")\n",
    "            StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['config'].str.contains(\":\", case=False,na=False),'config']=\"Until: \"+StartSchDay_Hourly_df['config'].astype(\"string\")\n",
    "        \n",
    "        StartSchDay_Hourly_df.loc[:,'Time_Tag']=StartSchDay_Hourly_df.loc[:,'Time_Tag'].ffill()\n",
    "        StartSchDay_Hourly_df['Time_Tag']=StartSchDay_Hourly_df['Time_Tag'].str.strip()\n",
    "        StartSchDay_Hourly_df=StartSchDay_Hourly_df[StartSchDay_Hourly_df['config'].str.contains(\"Until\",case=False,na=False)==False]\n",
    "        StartSchDay_Hourly_df['Time']=StartSchDay_Hourly_df['Time_Tag'].str.split(':',1).str[-1]\n",
    "        StartSchDay_Hourly_df[['Hour','Minute']]=StartSchDay_Hourly_df['Time'].str.split(':', expand=True) \n",
    "        \n",
    "        StartSchDay_Hourly_df['Hour']=StartSchDay_Hourly_df['Hour'].str.strip()\n",
    "        StartSchDay_Hourly_df['Minute']=StartSchDay_Hourly_df['Minute'].str.strip()\n",
    "    \n",
    "        StartSchDay_Hourly_df['Hour_Adjusted']=StartSchDay_Hourly_df['Hour'].str.replace('24','0')\n",
    "        StartSchDay_Hourly_df['Date']=Event_Start_Datetime.date()\n",
    "        StartSchDay_Hourly_df['Date']=StartSchDay_Hourly_df['Date'].astype(str)\n",
    "        StartSchDay_Hourly_df[\"DateTime\"]= pd.to_datetime(StartSchDay_Hourly_df['Date'].astype(str)+' '+ StartSchDay_Hourly_df[\"Hour_Adjusted\"].astype(str)+\":\"+StartSchDay_Hourly_df[\"Minute\"].astype(str),format='%Y-%m-%d %H:%M')\n",
    "        StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['Hour_Adjusted'] == '0', 'DateTime'] = StartSchDay_Hourly_df['DateTime']+ datetime.timedelta(days=1)\n",
    "        Start_Sch_Header_Name=StartSchDay_Header.loc[1,'config']\n",
    "\n",
    "        #extract rows before event start date/time\n",
    "        Sch_Before_EventStart=StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['DateTime']<Event_Start_Datetime]\n",
    "        Sch_Before_EventStart=Sch_Before_EventStart.reset_index(drop=True)\n",
    "        #extract rows after event start date/time\n",
    "        Sch_After_EventStart=StartSchDay_Hourly_df.loc[StartSchDay_Hourly_df['DateTime']>=Event_Start_Datetime]\n",
    "        Sch_After_EventStart=Sch_After_EventStart.reset_index(drop=True)\n",
    "\n",
    "        Disruptive_BeforeEventStart_Sch_Level=Sch_After_EventStart['config'].loc[0]\n",
    "        Disruptive_StartDayBeforeEventStart=pd.DataFrame(columns=['config','Time_Tag'])\n",
    "        if Event_StartTime != \"00:00\":\n",
    "            Disruptive_StartDayBeforeEventStart.loc[0]=[Disruptive_BeforeEventStart_Sch_Level,\"Until: {}\".format(Event_StartTime)]\n",
    "\n",
    "        EndSchDay_df=pd.DataFrame(list(EndSchDay.values())[0], columns=['config'])\n",
    "        EndSchDay_Header=EndSchDay_df[:4]\n",
    "        EndSchDay_Hourly_df=EndSchDay_df[4:].copy()\n",
    "        \n",
    "        if EndSchDay_Hourly_df['config'].str.contains(\"Until\", case=False,na=False).sum() >= 1:\n",
    "        \n",
    "            EndSchDay_Hourly_df.loc[EndSchDay_Hourly_df['config'].str.contains(\"Until\", case=False,na=False),'Time_Tag']=EndSchDay_Hourly_df['config']\n",
    "        else:\n",
    "            EndSchDay_Hourly_df.loc[EndSchDay_Hourly_df['config'].str.contains(\":\", case=False,na=False),'config']=\"Until: \"+EndSchDay_Hourly_df['config'].astype(\"string\")\n",
    "        \n",
    "        \n",
    "        EndSchDay_Hourly_df.loc[EndSchDay_Hourly_df['config'].str.contains(\"Until\", case=False,na=False),'Time_Tag']=EndSchDay_Hourly_df['config']\n",
    "        EndSchDay_Hourly_df.loc[:,'Time_Tag']=EndSchDay_Hourly_df.loc[:,'Time_Tag'].ffill()\n",
    "        EndSchDay_Hourly_df['Time_Tag']=EndSchDay_Hourly_df['Time_Tag'].str.strip()\n",
    "        EndSchDay_Hourly_df=EndSchDay_Hourly_df[EndSchDay_Hourly_df['config'].str.contains(\"Until\",case=False,na=False)==False]\n",
    "        EndSchDay_Hourly_df['Time']=EndSchDay_Hourly_df['Time_Tag'].str.split(':',1).str[-1]\n",
    "        EndSchDay_Hourly_df[['Hour','Minute']]=EndSchDay_Hourly_df['Time'].str.split(':', expand=True) \n",
    "        \n",
    "        EndSchDay_Hourly_df['Hour']=EndSchDay_Hourly_df['Hour'].str.strip()\n",
    "        EndSchDay_Hourly_df['Minute']=EndSchDay_Hourly_df['Minute'].str.strip()\n",
    "    \n",
    "        EndSchDay_Hourly_df['Hour_Adjusted']=EndSchDay_Hourly_df['Hour'].str.replace('24','0')\n",
    "        EndSchDay_Hourly_df['Date']=Event_End_Datetime.date()\n",
    "        EndSchDay_Hourly_df['Date']=EndSchDay_Hourly_df['Date'].astype(str)\n",
    "        EndSchDay_Hourly_df[\"DateTime\"]= pd.to_datetime(EndSchDay_Hourly_df['Date'].astype(str)+' '+ EndSchDay_Hourly_df[\"Hour_Adjusted\"].astype(str)+\":\"+EndSchDay_Hourly_df[\"Minute\"].astype(str),format='%Y-%m-%d %H:%M')\n",
    "        EndSchDay_Hourly_df.loc[EndSchDay_Hourly_df['Hour_Adjusted'] == '0', 'DateTime'] = EndSchDay_Hourly_df['DateTime']+ datetime.timedelta(days=1)\n",
    "        End_Sch_Header_Name=EndSchDay_Header.loc[1,'config']\n",
    "\n",
    "\n",
    "        #extract rows after event end date/time\n",
    "        Sch_After_EventEnd=EndSchDay_Hourly_df.loc[EndSchDay_Hourly_df['DateTime']>=Event_End_Datetime]\n",
    "\n",
    "\n",
    "        #Assemble Day Schedule for different disruptive modes\n",
    "        #Event start and end on same day\n",
    "        if DisruptiveMode == \"Same Day\":\n",
    "\n",
    "            Disruptive_EventSameDay=pd.DataFrame(columns=['config','Time_Tag'])\n",
    "            Disruptive_EventSameDay.loc[0]=[Sch_Level,\"Until: {}\".format(Event_EndTime)]\n",
    "\n",
    "            Day_Sch_df=pd.concat([Sch_Before_EventStart,Disruptive_StartDayBeforeEventStart,Disruptive_EventSameDay,Sch_After_EventEnd])\n",
    "            Day_Sch_df=Day_Sch_df.rename(columns={\"config\":\"Value_Until_Time\"})\n",
    "            Day_Sch_df=Day_Sch_df.drop(columns=['Time','Hour','Minute','Hour_Adjusted','Date','DateTime']).reset_index(drop=True)\n",
    "            Day_Sch_df=Day_Sch_df[[\"Time_Tag\",\"Value_Until_Time\"]]\n",
    "            Day_Sch_df=Day_Sch_df.reset_index()\n",
    "            Day_Sch_df=Day_Sch_df.melt(id_vars=['index']).reset_index().sort_values(by=['index','level_0'])\n",
    "            Day_Sch_df[\"index\"]=Day_Sch_df[\"index\"]+1\n",
    "            Day_Sch_df.loc[Day_Sch_df['variable'].str.contains('Value_Until_Time', case=False),'variable']=Day_Sch_df['variable']+\"_\"+Day_Sch_df['index'].astype('string')\n",
    "            Day_Sch_df.loc[Day_Sch_df['variable'].str.contains('Time_Tag', case=False),'variable']=\"Time_\"+Day_Sch_df['index'].astype('string')\n",
    "            Day_Sch_df=Day_Sch_df.rename(columns={\"variable\":\"Key\",\"value\":\"config\"})\n",
    "\n",
    "            #Remove Existing TRMod schedules if it already exists\n",
    "\n",
    "            for x in list(Day_Sch_Interval_idf):\n",
    "                if x.Name == Start_Sch_Header_Name + \"_Day_TRMod\":\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            #Add Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"SCHEDULE:Day:Interval\")\n",
    "            newobject.Name= Start_Sch_Header_Name + \"_Day_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in Day_Sch_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]\n",
    "\n",
    "            ######################################Assemble Week Schedule#####################################\n",
    "\n",
    "            #Remove Existing TRMod schedules if it already exists \n",
    "            for x in list(Week_Sch_Compact_idf):\n",
    "                if x.Name == StartSchName + 'TRWeekSch_DisruptDay':\n",
    "                    print(\"Item {} removed\".format(x.Name))\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            #create new week schedule\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=StartSchName + 'TRWeekSch_DisruptDay'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']= Start_Sch_Header_Name + \"_Day_TRMod\"\n",
    "\n",
    "\n",
    "        if DisruptiveMode == \"Not Same Day - Multiple Days\" or DisruptiveMode == \"Not Same Day\":    \n",
    "            print(\"The event starts and ends on separate days\")\n",
    "\n",
    "            Disruptive_EventStartDay=pd.DataFrame(columns=['config','Time_Tag'])\n",
    "            Disruptive_EventStartDay.loc[0]=[Sch_Level,\"Until: 24:00\"]\n",
    "\n",
    "            StartDay_Sch_df=pd.concat([Sch_Before_EventStart,Disruptive_StartDayBeforeEventStart,Disruptive_EventStartDay])\n",
    "\n",
    "            StartDay_Sch_df=StartDay_Sch_df.rename(columns={\"config\":\"Value_Until_Time\"})\n",
    "            StartDay_Sch_df=StartDay_Sch_df.drop(columns=['Time','Hour','Minute','Hour_Adjusted','Date','DateTime']).reset_index(drop=True)\n",
    "            StartDay_Sch_df=StartDay_Sch_df[[\"Time_Tag\",\"Value_Until_Time\"]]\n",
    "            StartDay_Sch_df=StartDay_Sch_df.reset_index()\n",
    "            StartDay_Sch_df=StartDay_Sch_df.melt(id_vars=['index']).reset_index().sort_values(by=['index','level_0'])\n",
    "            StartDay_Sch_df[\"index\"]=StartDay_Sch_df[\"index\"]+1\n",
    "            StartDay_Sch_df.loc[StartDay_Sch_df['variable'].str.contains('Value_Until_Time', case=False),'variable']=StartDay_Sch_df['variable']+\"_\"+StartDay_Sch_df['index'].astype('string')\n",
    "            StartDay_Sch_df.loc[StartDay_Sch_df['variable'].str.contains('Time_Tag', case=False),'variable']=\"Time_\"+StartDay_Sch_df['index'].astype('string')\n",
    "            StartDay_Sch_df=StartDay_Sch_df.rename(columns={\"variable\":\"Key\",\"value\":\"config\"})\n",
    "\n",
    "            Disruptive_EventEndDay=pd.DataFrame(columns=['config','Time_Tag'])\n",
    "            Disruptive_EventEndDay.loc[0]=[Sch_Level,\"Until: {}\".format(Event_EndTime)]\n",
    "\n",
    "            EndDay_Sch_df=pd.concat([Disruptive_EventEndDay,Sch_After_EventEnd])\n",
    "\n",
    "            EndDay_Sch_df=EndDay_Sch_df.rename(columns={\"config\":\"Value_Until_Time\"})\n",
    "            EndDay_Sch_df=EndDay_Sch_df.drop(columns=['Time','Hour','Minute','Hour_Adjusted','Date','DateTime']).reset_index(drop=True)\n",
    "            EndDay_Sch_df=EndDay_Sch_df[[\"Time_Tag\",\"Value_Until_Time\"]]\n",
    "            EndDay_Sch_df=EndDay_Sch_df.reset_index()\n",
    "            EndDay_Sch_df=EndDay_Sch_df.melt(id_vars=['index']).reset_index().sort_values(by=['index','level_0'])\n",
    "            EndDay_Sch_df[\"index\"]=EndDay_Sch_df[\"index\"]+1\n",
    "            EndDay_Sch_df.loc[EndDay_Sch_df['variable'].str.contains('Value_Until_Time', case=False),'variable']=EndDay_Sch_df['variable']+\"_\"+EndDay_Sch_df['index'].astype('string')\n",
    "            EndDay_Sch_df.loc[EndDay_Sch_df['variable'].str.contains('Time_Tag', case=False),'variable']=\"Time_\"+EndDay_Sch_df['index'].astype('string')\n",
    "            EndDay_Sch_df=EndDay_Sch_df.rename(columns={\"variable\":\"Key\",\"value\":\"config\"})\n",
    "\n",
    "            Disruptive_EventBetweenDay=pd.DataFrame(columns=['config','Time_Tag'])\n",
    "            Disruptive_EventBetweenDay.loc[0]=[Sch_Level,\"Until: 24:00\"]\n",
    "\n",
    "            BetweenDay_Sch_df=Disruptive_EventBetweenDay\n",
    "\n",
    "            BetweenDay_Sch_df=BetweenDay_Sch_df.rename(columns={\"config\":\"Value_Until_Time\"})\n",
    "            #BetweenDay_Sch_df=BetweenDay_Sch_df.drop(columns=['Time','Hour','Minute','Hour_Adjusted','Date','DateTime']).reset_index(drop=True)\n",
    "            BetweenDay_Sch_df=BetweenDay_Sch_df[[\"Time_Tag\",\"Value_Until_Time\"]]\n",
    "            BetweenDay_Sch_df=BetweenDay_Sch_df.reset_index()\n",
    "            BetweenDay_Sch_df=BetweenDay_Sch_df.melt(id_vars=['index']).reset_index().sort_values(by=['index','level_0'])\n",
    "            BetweenDay_Sch_df[\"index\"]=BetweenDay_Sch_df[\"index\"]+1\n",
    "            BetweenDay_Sch_df.loc[BetweenDay_Sch_df['variable'].str.contains('Value_Until_Time', case=False),'variable']=BetweenDay_Sch_df['variable']+\"_\"+BetweenDay_Sch_df['index'].astype('string')\n",
    "            BetweenDay_Sch_df.loc[BetweenDay_Sch_df['variable'].str.contains('Time_Tag', case=False),'variable']=\"Time_\"+BetweenDay_Sch_df['index'].astype('string')\n",
    "            BetweenDay_Sch_df=BetweenDay_Sch_df.rename(columns={\"variable\":\"Key\",\"value\":\"config\"})\n",
    "\n",
    "            for x in list(Day_Sch_Interval_idf):\n",
    "                if (x.Name == Start_Sch_Header_Name + \"_StartDay_TRMod\") or (x.Name == End_Sch_Header_Name + \"_EndDay_TRMod\") or (x.Name == Sch_Type + \"_InBetweenSch_TRMod\"):\n",
    "                    print(\"Item {} removed\".format(x.Name))\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "            #Add Start Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"SCHEDULE:Day:Interval\")\n",
    "            newobject.Name= Start_Sch_Header_Name + \"_StartDay_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in StartDay_Sch_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]               \n",
    "\n",
    "\n",
    "            #Add End Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"SCHEDULE:Day:Interval\")\n",
    "            newobject.Name= End_Sch_Header_Name + \"_EndDay_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in EndDay_Sch_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]\n",
    "\n",
    "            #Add In-between Day Schedule\n",
    "\n",
    "            newobject = idf1.newidfobject(\"SCHEDULE:Day:Interval\")\n",
    "            newobject.Name= Sch_Type + \"_InBetweenSch_TRMod\"\n",
    "            newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "\n",
    "            for index, row in BetweenDay_Sch_df.iterrows():\n",
    "                field_name=row['Key']\n",
    "                newobject[field_name]=row[\"config\"]  \n",
    "\n",
    "\n",
    "            #######################################Assemble Week Schedules ##########################################    \n",
    "            #Remove Existing TRMod schedules if it already exists\n",
    "\n",
    "            for x in list(Week_Sch_Compact_idf):\n",
    "                if (x.Name == StartSchName + 'TRWeekSch_Start') or (x.Name == EndSchName + 'TRWeekSch_End') or (x.Name == Sch_Type + 'TRWeekSch_InBetween'):\n",
    "                    print(\"Item {} removed\".format(x.Name))\n",
    "                    idf1.removeidfobject(x)\n",
    "\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=StartSchName + 'TRWeekSch_Start'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']=Start_Sch_Header_Name + \"_StartDay_TRMod\"\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=EndSchName +'TRWeekSch_End'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']=End_Sch_Header_Name + \"_EndDay_TRMod\"\n",
    "\n",
    "            newobject = idf1.newidfobject(\"Schedule:Week:Compact\")\n",
    "            newobject['Name']=Sch_Type + 'TRWeekSch_InBetween'\n",
    "            newobject['DayType_List_1']='For AllDays'\n",
    "            newobject['ScheduleDay_Name_1']=Sch_Type + \"_InBetweenSch_TRMod\"\n",
    "\n",
    "\n",
    "\n",
    "    ###########################################Assemble Year Schedule############################################################\n",
    "\n",
    "\n",
    "    #extract rows before event start date/time - schedules that begin and end before event starts\n",
    "    Sch_Before_EventStart=Sch_df.loc[Sch_df['End_Date']<Event_Start_Datetime]\n",
    "\n",
    "    #extract rows after event start date/time - schedules that starts before event starts\n",
    "    Sch_After_EventStart=Sch_df.loc[(Sch_df['Start_Date']<=Event_Start_Datetime) & (Sch_df['End_Date']>=Event_Start_Datetime)]\n",
    "\n",
    "    Sch_After_EventStart_Mod=Sch_After_EventStart.copy()\n",
    "    Sch_After_EventStart_Mod['End_Date']=Event_Start_Datetime - datetime.timedelta(days=1)\n",
    "    Sch_After_EventStart_Mod['End_Month']=Sch_After_EventStart_Mod['End_Date'][0].month\n",
    "    Sch_After_EventStart_Mod['End_Day']=Sch_After_EventStart_Mod['End_Date'][0].day\n",
    "\n",
    "    #extract rows starts before event end date/time and ends after event end date/time\n",
    "    Sch_Between_EventEnd=Sch_df.loc[(Sch_df['Start_Date']<=Event_End_Datetime) & (Sch_df['End_Date']>=Event_End_Datetime)]\n",
    "\n",
    "    Sch_Between_EventEnd_Mod=Sch_Between_EventEnd.copy()\n",
    "    Sch_Between_EventEnd_Mod['Start_Date']=Event_End_Datetime + datetime.timedelta(days=1)\n",
    "    Sch_Between_EventEnd_Mod['Start_Month']=Sch_Between_EventEnd_Mod['Start_Date'][0].month\n",
    "    Sch_Between_EventEnd_Mod['Start_Day']=Sch_Between_EventEnd_Mod['Start_Date'][0].day\n",
    "\n",
    "    #extract rows after event end date/time\n",
    "    Sch_After_EventEnd=Sch_df.loc[(Sch_df['Start_Date']>Event_End_Datetime) & (Sch_df['End_Date']>Event_End_Datetime)]\n",
    "\n",
    "\n",
    "    #Assemble Distruptive Event\n",
    "\n",
    "    if DisruptiveMode == \"Same Day\":\n",
    "\n",
    "        Sch_Distruptive=Sch_After_EventStart_Mod.copy()\n",
    "        Sch_Distruptive['ScheduleWeek_Name']=StartSchName + 'TRWeekSch_DisruptDay'\n",
    "        Sch_Distruptive['Start_Date']=Event_Start_Datetime\n",
    "        Sch_Distruptive['End_Date']=Event_Start_Datetime\n",
    "        Sch_Distruptive['Start_Month']=Sch_Distruptive['Start_Date'][0].month\n",
    "        Sch_Distruptive['Start_Day']=Sch_Distruptive['Start_Date'][0].day\n",
    "        Sch_Distruptive['End_Month']=Sch_Distruptive['End_Date'][0].month\n",
    "        Sch_Distruptive['End_Day']=Sch_Distruptive['End_Date'][0].day\n",
    "\n",
    "        Sch_TR=pd.concat([Sch_After_EventStart_Mod,Sch_Distruptive,Sch_Between_EventEnd_Mod,Sch_After_EventEnd])\n",
    "\n",
    "\n",
    "    elif DisruptiveMode == \"Not Same Day\":\n",
    "        print(\"1\")\n",
    "        Sch_Distruptive_Start=Sch_After_EventStart_Mod.copy()\n",
    "        Sch_Distruptive_Start['ScheduleWeek_Name']=StartSchName + 'TRWeekSch_Start'\n",
    "        Sch_Distruptive_Start['Start_Date']=Event_Start_Datetime\n",
    "        Sch_Distruptive_Start['End_Date']=Event_Start_Datetime\n",
    "        Sch_Distruptive_Start['Start_Month']=Sch_Distruptive_Start['Start_Date'][0].month\n",
    "        Sch_Distruptive_Start['Start_Day']=Sch_Distruptive_Start['Start_Date'][0].day\n",
    "        Sch_Distruptive_Start['End_Month']=Sch_Distruptive_Start['End_Date'][0].month\n",
    "        Sch_Distruptive_Start['End_Day']=Sch_Distruptive_Start['End_Date'][0].day\n",
    "\n",
    "        Sch_Distruptive_End=Sch_After_EventStart_Mod.copy()\n",
    "        Sch_Distruptive_End['ScheduleWeek_Name']=EndSchName +'TRWeekSch_End'\n",
    "        Sch_Distruptive_End['Start_Date']=Event_End_Datetime\n",
    "        Sch_Distruptive_End['End_Date']=Event_End_Datetime\n",
    "        Sch_Distruptive_End['Start_Month']=Sch_Distruptive_End['Start_Date'][0].month\n",
    "        Sch_Distruptive_End['Start_Day']=Sch_Distruptive_End['Start_Date'][0].day\n",
    "        Sch_Distruptive_End['End_Month']=Sch_Distruptive_End['End_Date'][0].month\n",
    "        Sch_Distruptive_End['End_Day']=Sch_Distruptive_End['End_Date'][0].day\n",
    "\n",
    "        Sch_TR=pd.concat([Sch_After_EventStart_Mod,Sch_Distruptive_Start,Sch_Distruptive_End,Sch_Between_EventEnd_Mod,Sch_After_EventEnd])\n",
    "\n",
    "\n",
    "    elif DisruptiveMode == \"Not Same Day - Multiple Days\":\n",
    "        Sch_Distruptive_Start=Sch_After_EventStart_Mod.copy()\n",
    "        Sch_Distruptive_Start['ScheduleWeek_Name']=StartSchName + 'TRWeekSch_Start'\n",
    "        Sch_Distruptive_Start['Start_Date']=Event_Start_Datetime\n",
    "        Sch_Distruptive_Start['End_Date']=Event_Start_Datetime\n",
    "        Sch_Distruptive_Start['Start_Month']=Sch_Distruptive_Start['Start_Date'][0].month\n",
    "        Sch_Distruptive_Start['Start_Day']=Sch_Distruptive_Start['Start_Date'][0].day\n",
    "        Sch_Distruptive_Start['End_Month']=Sch_Distruptive_Start['End_Date'][0].month\n",
    "        Sch_Distruptive_Start['End_Day']=Sch_Distruptive_Start['End_Date'][0].day\n",
    "\n",
    "        Sch_Distruptive_InBetween=Sch_After_EventStart_Mod.copy()\n",
    "        Sch_Distruptive_InBetween['ScheduleWeek_Name']=Sch_Type + 'TRWeekSch_InBetween'\n",
    "        Sch_Distruptive_InBetween['Start_Date']=Event_Start_Datetime + datetime.timedelta(days=1)\n",
    "        Sch_Distruptive_InBetween['End_Date']=Event_End_Datetime + datetime.timedelta(days=-1)\n",
    "        Sch_Distruptive_InBetween['Start_Month']=Sch_Distruptive_InBetween['Start_Date'][0].month\n",
    "        Sch_Distruptive_InBetween['Start_Day']=Sch_Distruptive_InBetween['Start_Date'][0].day\n",
    "        Sch_Distruptive_InBetween['End_Month']=Sch_Distruptive_InBetween['End_Date'][0].month\n",
    "        Sch_Distruptive_InBetween['End_Day']=Sch_Distruptive_InBetween['End_Date'][0].day\n",
    "\n",
    "        Sch_Distruptive_End=Sch_After_EventStart_Mod.copy()\n",
    "        Sch_Distruptive_End['ScheduleWeek_Name']=EndSchName +'TRWeekSch_End'\n",
    "        Sch_Distruptive_End['Start_Date']=Event_End_Datetime\n",
    "        Sch_Distruptive_End['End_Date']=Event_End_Datetime\n",
    "        Sch_Distruptive_End['Start_Month']=Sch_Distruptive_End['Start_Date'][0].month\n",
    "        Sch_Distruptive_End['Start_Day']=Sch_Distruptive_End['Start_Date'][0].day\n",
    "        Sch_Distruptive_End['End_Month']=Sch_Distruptive_End['End_Date'][0].month\n",
    "        Sch_Distruptive_End['End_Day']=Sch_Distruptive_End['End_Date'][0].day\n",
    "\n",
    "        Sch_TR=pd.concat([Sch_After_EventStart_Mod,Sch_Distruptive_Start,Sch_Distruptive_InBetween,Sch_Distruptive_End,Sch_Between_EventEnd_Mod,Sch_After_EventEnd])\n",
    "\n",
    "\n",
    "    Sch_TR=Sch_TR.reset_index(drop=True) \n",
    "    Sch_TR=Sch_TR.reset_index()\n",
    "    Sch_TR['index']=Sch_TR['index']+1\n",
    "\n",
    "    Sch_TR=Sch_TR.drop(columns=['Start_Date','End_Date'])\n",
    "    Sch_TR=Sch_TR.melt(id_vars=['index'])\n",
    "    Sch_TR['Key']=Sch_TR['Key']+\"_\"+Sch_TR['index'].astype(str)\n",
    "    Sch_TR=Sch_TR.reset_index()\n",
    "\n",
    "    Sch_TR=Sch_TR.sort_values(by=['index', 'level_0'])\n",
    "    Sch_TR=Sch_TR.drop(columns=['level_0','index'])\n",
    "\n",
    "    #Remove Existing TRMod schedules if it already exists\n",
    "    for x in list(Schedule_Year):\n",
    "        if x.Name == Sch_Header_Name+\"_TRMod\":\n",
    "            idf1.removeidfobject(x)\n",
    "\n",
    "\n",
    "    newobject = idf1.newidfobject(\"Schedule:Year\")\n",
    "    newobject['Name']=Sch_Header_Name+\"_TRMod\"\n",
    "    if Sch_Type == \"Zone Control\":\n",
    "        newobject.Schedule_Type_Limits_Name=\"Control Type\"\n",
    "        \n",
    "    elif (Sch_Type == \"Heating_Setpoint\") or (Sch_Type == \"Cooling_Setpoint\"):\n",
    "        newobject.Schedule_Type_Limits_Name=\"Temperature\"\n",
    "        \n",
    "    else:\n",
    "        newobject.Schedule_Type_Limits_Name=\"Fraction\"\n",
    "    \n",
    "    \n",
    "    for index, row in Sch_TR.iterrows():\n",
    "        field_name=row['Key']\n",
    "        newobject[field_name]=row[\"value\"]\n",
    "\n",
    "    print(\"new schedule added\"+Sch_Header_Name+\"_TRMod\")\n",
    "    \n",
    "    return(newobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScheduleConvertConstant(Schedule_Name,Type):\n",
    "\n",
    "######Purpose of this function is to convert constant schedules#################################################################################################\n",
    "######Input of the function is the schedule name and the type of schedule #####################################################################################\n",
    "######Output of this function is the schedule object being added to the idf object. As part of the function, the schedule object is added to the idf object ###\n",
    "    \n",
    "    \n",
    "    global Schedule\n",
    "    global dict_items\n",
    "\n",
    "    Sch_Type=Type\n",
    "    if Sch_Type == \"Occupancy\":\n",
    "        Sch_Level=\"1\"\n",
    "\n",
    "    elif Sch_Type == \"Equipment\":\n",
    "        Sch_Level=\"0\"\n",
    "\n",
    "    elif Sch_Type == \"Zone Control\":\n",
    "        Sch_Level=\"0\"\n",
    "\n",
    "    elif Sch_Type == \"Zone Control\":\n",
    "        Sch_Level=\"0\"\n",
    "\n",
    "    ### Logic for different end date/time\n",
    "    Disruptive_TimeDelta = Event_End_Datetime.date() - Event_Start_Datetime.date()\n",
    "    Disruptive_TimeDelta_Days=Disruptive_TimeDelta.days    \n",
    "\n",
    "    if Event_End_Datetime.date() == Event_Start_Datetime.date():\n",
    "        DisruptiveMode = \"Same Day\"\n",
    "    elif Disruptive_TimeDelta_Days < 2:\n",
    "        DisruptiveMode = \"Not Same Day\"\n",
    "    else:\n",
    "        DisruptiveMode = \"Not Same Day - Multiple Days\"    \n",
    "\n",
    "    Schedule_Constant=idf1.idfobjects['SCHEDULE:CONSTANT']    \n",
    "\n",
    "    Event_DayBeforeStart=Event_Start_Datetime.date() + datetime.timedelta(days=-1)\n",
    "    Event_DayBeforeEnd=Event_End_Datetime.date() + datetime.timedelta(days=-1)\n",
    "\n",
    "    Event_DayBeforeStart_idfformat=\"{}/{:02d}\".format(Event_DayBeforeStart.month,Event_DayBeforeStart.day)\n",
    "    Event_DayBeforeEnd_idfformat=\"{}/{:02d}\".format(Event_DayBeforeEnd.month,Event_DayBeforeEnd.day)\n",
    "\n",
    "\n",
    "    for x in Schedule_Constant:\n",
    "     ##* revised after test 1\n",
    "         if x.Name == Schedule_Name:\n",
    "             ScheduelType_Limits_Name=x['Schedule_Type_Limits_Name']\n",
    "             ScheduelType_hourly_Level=x['Hourly_Value']\n",
    "\n",
    "    ##logic for the day before                \n",
    "    DayBeforeEventSch=pd.DataFrame(columns=['config'])\n",
    "    DayBeforeEventSch.loc[0]=[\"Through: {}\".format(Event_DayBeforeStart_idfformat)]\n",
    "    DayBeforeEventSch.loc[1]=[\"For: AllDays\"]\n",
    "    DayBeforeEventSch.loc[2]=[\"Until: 24:00\"]\n",
    "    DayBeforeEventSch.loc[3]=[ScheduelType_hourly_Level]\n",
    "\n",
    "    ##logic for the start day\n",
    "    EventStartDaySch=pd.DataFrame(columns=['config'])\n",
    "    EventStartDaySch.loc[0]=[\"Through: {}\".format(Event_StartDate)]\n",
    "    EventStartDaySch.loc[1]=[\"For: AllDays\"]\n",
    "    EventStartDaySch.loc[2]=[\"Until: {}\".format(Event_StartTime)]\n",
    "    EventStartDaySch.loc[3]=[ScheduelType_hourly_Level]\n",
    "\n",
    "    if not Event_StartTime == \"24:00\":\n",
    "        EventStartDaySch.loc[4]=[\"Until: 24:00\"]\n",
    "        EventStartDaySch.loc[5]=[Sch_Level]\n",
    "\n",
    "    ##logic for day before end       \n",
    "    EventInBetweenDaySch=pd.DataFrame(columns=['config'])\n",
    "    EventInBetweenDaySch.loc[0]=[\"Through: {}\".format(Event_DayBeforeEnd_idfformat)]\n",
    "    EventInBetweenDaySch.loc[1]=[\"For: AllDays\"]\n",
    "    EventInBetweenDaySch.loc[2]=[\"Until: 24:00\"]\n",
    "    EventInBetweenDaySch.loc[3]=[Sch_Level]\n",
    "\n",
    "\n",
    "    EndEventDaySch=pd.DataFrame(columns=['config'])\n",
    "    EndEventDaySch.loc[0]=[\"Through: {}\".format(Event_EndDate)]\n",
    "    EndEventDaySch.loc[1]=[\"For: AllDays\"]\n",
    "    EndEventDaySch.loc[2]=[\"Until: {}\".format(Event_EndTime)]\n",
    "    EndEventDaySch.loc[3]=[Sch_Level]    \n",
    "    if not Event_EndTime == \"24:00\":\n",
    "        EndEventDaySch.loc[4]=[\"Until: 24:00\"]\n",
    "        EndEventDaySch.loc[5]=[ScheduelType_hourly_Level]\n",
    "\n",
    "    AfterEndEventDaySch=pd.DataFrame(columns=['config'])\n",
    "    AfterEndEventDaySch.loc[0]=[\"Through: 12/31\"]\n",
    "    AfterEndEventDaySch.loc[1]=[\"For: AllDays\"]\n",
    "    AfterEndEventDaySch.loc[2]=[\"Until: 24:00\"]\n",
    "    AfterEndEventDaySch.loc[3]=[ScheduelType_hourly_Level]  \n",
    "\n",
    "    if DisruptiveMode == \"Same Day\":\n",
    "        EventSameDaySch=pd.DataFrame(columns=['config'])\n",
    "        EventSameDaySch.loc[0]=[\"Through: {}\".format(Event_StartDate)]\n",
    "        EventSameDaySch.loc[1]=[\"For: AllDays\"]\n",
    "        EventSameDaySch.loc[2]=[\"Until: {}\".format(Event_StartTime)]\n",
    "        EventSameDaySch.loc[3]=[ScheduelType_hourly_Level]\n",
    "        EventSameDaySch.loc[4]=[\"Until: {}\".format(Event_EndTime)]\n",
    "        EventSameDaySch.loc[5]=[Sch_Level]\n",
    "        if not Event_EndTime == \"24:00\":\n",
    "            EventSameDaySch.loc[6]=[\"Until: 24:00\"]\n",
    "            EventSameDaySch.loc[7]=[ScheduelType_hourly_Level]\n",
    "\n",
    "\n",
    "        Sch_df=pd.concat([DayBeforeEventSch,EventSameDaySch,AfterEndEventDaySch])\n",
    "\n",
    "    if DisruptiveMode == \"Not Same Day\":\n",
    "\n",
    "        Sch_df=pd.concat([DayBeforeEventSch,EventStartDaySch,EndEventDaySch,AfterEndEventDaySch])\n",
    "\n",
    "    if DisruptiveMode == \"Not Same Day - Multiple Days\":\n",
    "\n",
    "        Sch_df=pd.concat([DayBeforeEventSch,EventStartDaySch,EventInBetweenDaySch,EndEventDaySch,AfterEndEventDaySch])\n",
    "\n",
    "    Sch_df=Sch_df.reset_index(drop=True).reset_index()\n",
    "    Sch_df['index']=\"Field_\"+(Sch_df['index']+1).astype('string')\n",
    "\n",
    "\n",
    "    Schedule_Compact=idf1.idfobjects['SCHEDULE:COMPACT']    \n",
    "    #Remove Existing TRMod schedules if it already exists\n",
    "    for x in list(Schedule_Compact):\n",
    "        if x.Name == Schedule_Name +\"_TRMod\":\n",
    "            idf1.removeidfobject(x)\n",
    "\n",
    "    newobject = idf1.newidfobject(\"SCHEDULE:COMPACT\")\n",
    "    newobject['Name']=Schedule_Name +\"_TRMod\"\n",
    "    newobject['Schedule_Type_Limits_Name']= ScheduelType_Limits_Name\n",
    "\n",
    "    for index, row in Sch_df.iterrows():\n",
    "        field_name=row['index']\n",
    "        newobject[field_name]=row[\"config\"]\n",
    "\n",
    "    print(\"new schedule added\"+Schedule_Name+\"_TRMod\")\n",
    "\n",
    "    return(newobject)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28535516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScheduleConvert(Schedule_Name,Schedule_Function_Type):\n",
    "\n",
    "######Purpose of this function is to execute schedule conversion by selecting the correct schedule conversion function##############################################################\n",
    "######Input of the function is the schedule name and the type of schedule ##########################################################################################################\n",
    "######Output of this function is the schedule object being added to the idf object. As part of the conversion function execution, the schedule object is added to the idf object ###\n",
    "\n",
    "    Schedule_Name=Schedule_Name\n",
    "    Schedule_Function_Type=Schedule_Function_Type\n",
    "    Sch_List=ListOfSchedules(idf1)\n",
    "\n",
    "    Schedule_Type=Sch_List.loc[Sch_List['Name']==Schedule_Name,'Type'].iloc[0]\n",
    "\n",
    "    if Schedule_Type.upper() == \"SCHEDULE:YEAR\":\n",
    "        newobject=ScheduleConvertYearly(Schedule_Name,Schedule_Function_Type)\n",
    "\n",
    "    elif Schedule_Type.upper() == \"SCHEDULE:COMPACT\":\n",
    "        newobject=ScheduleConvertCompact(Schedule_Name,Schedule_Function_Type)\n",
    "        \n",
    "    elif Schedule_Type.upper() == \"SCHEDULE:CONSTANT\":\n",
    "        newobject=ScheduleConvertConstant(Schedule_Name,Schedule_Function_Type)\n",
    "    \n",
    "    return(newobject)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf5600",
   "metadata": {},
   "source": [
    "# Convert and Apply Modified Schedules to Occupancy and Equipment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create modified occupancy schedules\n",
    "ZoneList=ListOfZoneOccupantSchedule(idf1)\n",
    "ZoneList_OccSch=ZoneList['Occupancy Schedule'].unique()\n",
    "ZoneList_OccSch\n",
    "\n",
    "for sch in ZoneList_OccSch:\n",
    "    ScheduleConvert(sch,\"Occupancy\")\n",
    "    \n",
    "#Assign Occupancy Schedules \n",
    "OccupancyObjects=idf1.idfobjects['People']\n",
    "for object in OccupancyObjects:\n",
    "    if \"_TRmod\" not in object.Number_of_People_Schedule_Name:\n",
    "        object.Number_of_People_Schedule_Name=object.Number_of_People_Schedule_Name+\"_TRmod\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3558c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following steps are to apply schedule conversion to different equipment, additional equipment can be applied to the apporpriate list.\n",
    "\n",
    "#Creating a list of Schedules\n",
    "\n",
    "Sch_List=ListOfSchedules(idf1)\n",
    "Sch_List['Name_Lower']=Sch_List['Name'].str.lower()\n",
    "\n",
    "##########################################  Electric Equipment  #############################################   \n",
    "Equipment_List=[\n",
    "'ElectricEquipment',\n",
    "'Lights',\n",
    "'Exterior:Lights',\n",
    "'GasEquipment',\n",
    "'OtherEquipment',\n",
    "'Exterior:FuelEquipment'\n",
    "]\n",
    "\n",
    "Equipment_Sch_Name_List=[]\n",
    "\n",
    "for equipment in Equipment_List:\n",
    "    try:\n",
    "        equip_idf=idf1.idfobjects[equipment]    \n",
    "        for e in equip_idf: \n",
    "            Equipment_Sch_Name_List.append(e.Schedule_Name)\n",
    "\n",
    "            if (not e.Schedule_Name.strip()=='') and (not e.Schedule_Name.endswith(\"TRmod\")): \n",
    "                e.Schedule_Name=e.Schedule_Name+\"_TRmod\"\n",
    "    except:\n",
    "        continue       \n",
    "\n",
    "Equipment_Sch_Name_List=list(set(Equipment_Sch_Name_List))\n",
    "\n",
    "Equipment_Sch_List=pd.DataFrame({'Schedule Name':Equipment_Sch_Name_List})\n",
    "\n",
    "Equipment_Sch_List=Equipment_Sch_List.merge(Sch_List,left_on='Schedule Name', right_on='Name').drop(columns='Name')\n",
    "\n",
    "\n",
    "for equip_ind in Equipment_Sch_List.index:\n",
    "       \n",
    "    Sch_name=Equipment_Sch_List['Schedule Name'][equip_ind]\n",
    "    ScheduleConvert(Sch_name,\"Equipment\") \n",
    "\n",
    "####################################################  HVAC  #################################################        \n",
    "\n",
    "#HVAC Equipment\n",
    "HVAC_Equipment_List=[\n",
    "'ZoneHVAC:EnergyRecoveryVentilator',\n",
    "'AirTerminal:SingleDuct:Uncontrolled',\n",
    "'Fan:OnOff',\n",
    "'AirLoopHVAC:Unitary:Furnace:HeatCool',\n",
    "'Airloophvac:Unitaryheatpump:Watertoair',\n",
    "'Fan:VariableVolume',\n",
    "'Fan:ConstantVolume',\n",
    "'ElectricLoadCenter:Transformer'\n",
    "]\n",
    "\n",
    "HVAC_Equipment_Sch_Name_List=[]\n",
    "\n",
    "for equipment in HVAC_Equipment_List:\n",
    "\n",
    "    ##* added after first test\n",
    "    try:\n",
    "    \n",
    "        equip_idf=idf1.idfobjects[equipment]    \n",
    "        for e in equip_idf: \n",
    "            \n",
    "            if (not e.Availability_Schedule_Name.strip()=='') and (not e.Availability_Schedule_Name.endswith(\"TRmod\")):\n",
    "                HVAC_Equipment_Sch_Name_List.append(e.Availability_Schedule_Name)\n",
    "                e.Availability_Schedule_Name=e.Availability_Schedule_Name+\"_TRmod\"\n",
    "                #print(e.Availability_Schedule_Name)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        continue\n",
    "        \n",
    "HVAC_Equipment_Sch_Name_List=list(set(HVAC_Equipment_Sch_Name_List))\n",
    "\n",
    "HVAC_Equipment_Sch_List=pd.DataFrame({'Schedule Name':HVAC_Equipment_Sch_Name_List})\n",
    "HVAC_Equipment_Sch_List['Schedule Name Lower']=HVAC_Equipment_Sch_List['Schedule Name'].str.lower()\n",
    "\n",
    "HVAC_Equipment_Sch_List=HVAC_Equipment_Sch_List.merge(Sch_List,left_on='Schedule Name Lower', right_on='Name_Lower')\n",
    "\n",
    "for equip_ind in HVAC_Equipment_Sch_List.index:\n",
    "    \n",
    "    Sch_name=HVAC_Equipment_Sch_List['Name'][equip_ind]\n",
    "    \n",
    "    #print(Sch_name)\n",
    "    ScheduleConvert(Sch_name,\"Equipment\") \n",
    "\n",
    "#Simple Ventilation\n",
    "vent=idf1.idfobjects['ZoneVentilation:DesignFlowRate']      \n",
    "\n",
    "vent_name_list=[]\n",
    "for v in vent:\n",
    "    vent_name_list.append(v.Schedule_Name)\n",
    "    if not v.Schedule_Name.endswith(\"TRmod\"):\n",
    "        v.Schedule_Name=v.Schedule_Name+\"_TRmod\"\n",
    "    \n",
    "Vent_Sch_List=pd.DataFrame({'Schedule Name':vent_name_list})\n",
    "\n",
    "Vent_Sch_List=Vent_Sch_List.merge(Sch_List,left_on='Schedule Name', right_on='Name').drop(columns='Name')\n",
    "\n",
    "for vent_ind in Vent_Sch_List.index:\n",
    "    \n",
    "    Sch_name=Vent_Sch_List['Schedule Name'][vent_ind]\n",
    "    #print(Sch_name)\n",
    "    ScheduleConvert(Sch_name,\"Equipment\") \n",
    "\n",
    "#HVAC Control\n",
    "\n",
    "HVAC_Control=idf1.idfobjects['ZoneControl:Thermostat']      \n",
    "\n",
    "HVAC_Control_name_list=[]\n",
    "for h in HVAC_Control:\n",
    "    HVAC_Control_name_list.append(h.Control_Type_Schedule_Name)\n",
    "    if not h.Control_Type_Schedule_Name.endswith(\"TRmod\"):\n",
    "        h.Control_Type_Schedule_Name=h.Control_Type_Schedule_Name+\"_TRmod\"\n",
    "    \n",
    "HVAC_Control_List=pd.DataFrame({'Schedule Name':HVAC_Control_name_list})\n",
    "\n",
    "HVAC_Control_List=HVAC_Control_List.merge(Sch_List,left_on='Schedule Name', right_on='Name').drop(columns='Name')\n",
    "\n",
    "for hvac_ind in HVAC_Control_List.index:\n",
    "    \n",
    "    Sch_name=HVAC_Control_List['Schedule Name'][hvac_ind]\n",
    "    ScheduleConvert(Sch_name,\"Zone Control\") \n",
    "\n",
    "####################################################  WaterUse  #################################################       \n",
    "    \n",
    "#WaterUse\n",
    "water=idf1.idfobjects['Wateruse:Equipment']      \n",
    "\n",
    "water_name_list=[]\n",
    "for w in water:\n",
    "    water_name_list.append(w.Flow_Rate_Fraction_Schedule_Name)\n",
    "    if not w.Flow_Rate_Fraction_Schedule_Name.endswith(\"TRmod\"):\n",
    "        w.Flow_Rate_Fraction_Schedule_Name=w.Flow_Rate_Fraction_Schedule_Name+\"_TRmod\"\n",
    "    \n",
    "Water_Sch_List=pd.DataFrame({'Schedule Name':water_name_list})\n",
    "\n",
    "Water_Sch_List=Water_Sch_List.merge(Sch_List,left_on='Schedule Name', right_on='Name').drop(columns='Name')\n",
    "\n",
    "for water_ind in Water_Sch_List.index:\n",
    "    \n",
    "    Sch_name=Water_Sch_List['Schedule Name'][water_ind]\n",
    "    #print(Sch_name)\n",
    "    ScheduleConvert(Sch_name,\"Equipment\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display schedule list\n",
    "sch_list=ListOfSchedules(idf1)\n",
    "sch_list\n",
    "display(HTML(sch_list.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c844c",
   "metadata": {},
   "source": [
    "# Set Output Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95756902",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputVariables=[\"Zone Air Temperature\",\n",
    "                 \"Site Outdoor Air Drybulb Temperature\",\n",
    "                 \"Zone People Occupant Count\", \n",
    "                 \"Zone Air Heat Balance System Air Transfer Rate\",\n",
    "                 \"Zone Ventilation Fan Electric Energy\",\n",
    "                 \"Zone Air Heat Balance System Convective Heat Gain Rate\",\n",
    "                 \"Zone Mechanical Ventilation Air Changes per Hour\",\n",
    "                 \"Facility Total Building Electric Demand Power\",\n",
    "                 \"Facility Total HVAC Electric Demand Power\",\n",
    "                 \"Facility Total Electric Demand Power\",\n",
    "                 \"Facility Total Building Electricity Demand Rate\",\n",
    "                 \"Facility Total HVAC Electricity Demand Rate\",\n",
    "                 \"Facility Total Electricity Demand Rate\",\n",
    "                 \"Lights Electric Power\",\n",
    "                 \"Lights Electricity Rate\"\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "for v in OutputVariables:\n",
    "    variable=v\n",
    "    OutputVariable=idf1.idfobjects[\"OUTPUT:VARIABLE\"]\n",
    "\n",
    "    matching = [v for v in idf1.idfobjects[\"OUTPUT:VARIABLE\"] if v.Variable_Name == variable]\n",
    "\n",
    "    if not OutputVariable:\n",
    "        print(\"Output variable list was empty, new Output Variable {} added\".format(variable))\n",
    "        newobject = idf1.newidfobject(\"OUTPUT:VARIABLE\")\n",
    "        newobject.Key_Value=\"*\"\n",
    "        newobject.Variable_Name=variable\n",
    "        newobject.Reporting_Frequency='Hourly'\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        if not matching:\n",
    "            print(\"New Output Variable {} added\".format(variable))\n",
    "            newobject = idf1.newidfobject(\"OUTPUT:VARIABLE\")\n",
    "            newobject.Key_Value=\"*\"\n",
    "            newobject.Variable_Name=variable\n",
    "            newobject.Reporting_Frequency='Hourly'\n",
    "            \n",
    "        else:\n",
    "        \n",
    "             for obj in OutputVariable:\n",
    "                if obj.Variable_Name == variable:\n",
    "                    print(\"Varible {} already in model, modified specification for thermal resilience analysis\".format(variable))\n",
    "                    obj.Key_Value=\"*\"\n",
    "                    obj.Reporting_Frequency='Hourly'\n",
    "                \n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a388c6",
   "metadata": {},
   "source": [
    "# Remove Day Light Saving Time Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805cfcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daylight saving time logic creates error at the midnight time step on the day disruptive event occurs\n",
    "\n",
    "idf_DaylightSavings=idf1.idfobjects['RUNPERIODCONTROL:DAYLIGHTSAVINGTIME']\n",
    "\n",
    "\n",
    "idf_DaylightSavings_len=len(idf_DaylightSavings)\n",
    "i=0\n",
    "while i < idf_DaylightSavings_len:\n",
    "    idf_DaylightSavings.pop(i)\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4cda3",
   "metadata": {},
   "source": [
    "# Save converted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecdf4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=save_idf_file(output_filename,idf_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96ae29",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c96ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read output file\n",
    "\n",
    "RunPeriod_Year=2023\n",
    "\n",
    "# the folder path should be a folder with csv files that contains hourly energy plus simulation outputs\n",
    "# multiple hourly output csv files can be in the folder for different simualtion scenarios, the script joins the results in one data table\n",
    "FolderPath=r\"C:\\Users\\Desktop\\Output\"\n",
    "FolderPath_Files = glob.glob(os.path.join(FolderPath , \"*.csv\"))\n",
    "\n",
    "ListofFiles=[]\n",
    "idf_output_df=[]\n",
    "df=[]\n",
    "\n",
    "for files in FolderPath_Files:\n",
    "    df=pd.read_csv(files, index_col=None, header=0)\n",
    "    df.insert(loc=1, column='FileName',value=os.path.basename(files))\n",
    "    ListofFiles.append(df)\n",
    "\n",
    "idf_output_df=pd.concat(ListofFiles,axis=0, ignore_index=True)\n",
    "idf_output_df_original=idf_output_df\n",
    "\n",
    "idf_output_df.insert(loc=1, column='Time', value=idf_output_df['Date/Time'].str.split(' ').str[-1])\n",
    "idf_output_df.insert(loc=2, column='Hour', value=idf_output_df['Time'].str.split(':').str[0])\n",
    "idf_output_df.insert(loc=3, column='Hour_Adjusted', value=idf_output_df['Hour'].replace('24','0'))\n",
    "idf_output_df.insert(loc=4, column='Minute', value=idf_output_df['Time'].str.split(':').str[1])\n",
    "idf_output_df.insert(loc=5, column='Date', value=idf_output_df['Date/Time'].str.split(' ').str[1])\n",
    "idf_output_df.insert(loc=6, column='Month', value=idf_output_df['Date'].str.split('/').str[0])\n",
    "idf_output_df.insert(loc=7, column='Day', value=idf_output_df['Date'].str.split('/').str[-1])\n",
    "\n",
    "idf_output_df.insert(loc=8, column='DateTime', value=pd.to_datetime(str(RunPeriod_Year)+\"-\"+idf_output_df['Month']+\"-\"+idf_output_df['Day']+' '+ idf_output_df[\"Hour_Adjusted\"]+\":\"+idf_output_df[\"Minute\"],format='%Y-%m-%d %H:%M'))\n",
    "\n",
    "idf_output_df.loc[idf_output_df['Hour_Adjusted'] == '0', 'DateTime'] = idf_output_df['DateTime']+ datetime.timedelta(days=1)\n",
    "\n",
    "idf_output_df=idf_output_df.melt(id_vars=['FileName','Date/Time','Time','Hour','Hour_Adjusted','Minute','Date','Month','Day','DateTime'])\n",
    "\n",
    "idf_output_df.insert(loc=10, column='Zone', value=idf_output_df['variable'].str.split(':').str[0])\n",
    "idf_output_df.insert(loc=11, column='Metric', value=idf_output_df['variable'].str.split(':').str[-1])\n",
    "\n",
    "idf_output_df['Metric']=idf_output_df['Metric'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c680d2f",
   "metadata": {},
   "source": [
    "# Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analytics Dashboard\n",
    "app=dash.Dash()\n",
    "Scenario_Selection_List=idf_output_df['FileName'].unique()\n",
    "Zone_Selection_List=idf_output_df['Zone'].unique()\n",
    "Metric_Selection_List=idf_output_df['Metric'].unique()\n",
    "\n",
    "\n",
    "app.layout=dcc.Tabs([\n",
    "    \n",
    "            #Thermal Resilience Analytics Dashboard\n",
    "            dcc.Tab(label='Thermal Resilience Analytics Dashboard', children=[\n",
    "            html.Div([html.H1('Thermal Resilience Analytics Dashboard'),\n",
    "            \n",
    "            html.Label(\"Select Simulation Scenario\"),\n",
    "            dcc.Dropdown(id='Simulation_Scenario',options=Scenario_Selection_List),\n",
    "            \n",
    "            html.Label(\"Select Zone\"),\n",
    "            dcc.Dropdown(id='Zone_Selection',options=Zone_Selection_List),\n",
    "            dcc.Tabs([\n",
    "            \n",
    "            dcc.Tab(label='Simulation Results Summary', children=[\n",
    "            html.Div([\n",
    "            html.Label(\"Enter Upper Threshold  \"),\n",
    "            dcc.Input(id='UpperThreshold_summary',value=25,type='number'),\n",
    "            html.Label(\"Enter Lower Threshold  \"),\n",
    "            dcc.Input(id='LowerThreshold_summary',value=18,type='number')]),            \n",
    "                \n",
    "                html.Div(\n",
    "            dash_table.DataTable(id='filtered_summary',data=[],columns=[]))    \n",
    "                     \n",
    "            ]),    \n",
    "                \n",
    "            dcc.Tab(label='Metric-Time Graph', children=[\n",
    "            html.Div([\n",
    "            html.Label(\"Select Metric\"),\n",
    "            dcc.Dropdown(id='Metric_Selection',options=Metric_Selection_List)]),\n",
    "            html.Div([\n",
    "            html.Label(\"Enter Upper Threshold  \"),\n",
    "            dcc.Input(id='UpperThreshold',value=25,type='number'),\n",
    "            html.Label(\"Enter Lower Threshold  \"),\n",
    "            dcc.Input(id='LowerThreshold',value=18,type='number')]),\n",
    "            html.Div(dcc.Graph(id='temp_graph'))          \n",
    "            ]),\n",
    "            \n",
    "            dcc.Tab(label='Carpet Plot', children=[    \n",
    "            html.Div([\n",
    "            html.Div([\n",
    "            html.Label(\"Enter Upper Threshold  \"),\n",
    "            dcc.Input(id='UpperThreshold_DegHr',value=25,type='number'),\n",
    "            html.Label(\"Enter Lower Threshold  \"),\n",
    "            dcc.Input(id='LowerThreshold_DegHr',value=18,type='number')]),       \n",
    "            ]),\n",
    "            dcc.Graph(id='carpet_plot'),\n",
    "            html.Div([\n",
    "            dash_table.DataTable(id='filtered_TA_summary',data=[],columns=[])\n",
    "            ])\n",
    "               \n",
    "            ])\n",
    "            \n",
    "            ])    \n",
    "                \n",
    "            ])\n",
    "            ])\n",
    "            ])\n",
    "            \n",
    "                     \n",
    "                \n",
    "        \n",
    "@app.callback(Output('temp_graph','figure'),\n",
    "             [Input('Zone_Selection','value'),\n",
    "             Input('Metric_Selection','value'),\n",
    "             Input('Simulation_Scenario','value'),\n",
    "             Input('UpperThreshold','value'),\n",
    "             Input('LowerThreshold','value')\n",
    "             ])\n",
    "\n",
    "def update_figure(selected_zone, selected_metric,selected_scenario, upper_temp, lower_temp):\n",
    "    \n",
    "    filtered_idf_output_df1=idf_output_df[(idf_output_df['Zone']==selected_zone)&(idf_output_df['FileName']==selected_scenario) & (idf_output_df['Metric']==selected_metric)]\n",
    "    fig=plty.graph_objects.Figure()\n",
    "    fig.add_trace(plty.graph_objects.Scatter(x=filtered_idf_output_df1.DateTime,y=filtered_idf_output_df1.value))\n",
    "    fig.update_xaxes(title_text='Time')\n",
    "    fig.update_yaxes(title_text='{} [\\u00b0 C]'.format(selected_metric))\n",
    "    fig.add_hrect(y0=lower_temp, y1=upper_temp, line_width=0, fillcolor=\"green\", opacity=0.2,annotation_text=\"Upper Limit\", annotation_position=\"inside top right\")\n",
    "    fig.add_hrect(y0=lower_temp, y1=upper_temp, line_width=0, fillcolor=\"green\", opacity=0,annotation_text=\"Lower Limit\", annotation_position=\"inside bottom right\")\n",
    "    fig.add_vrect(x0=Event_Start_Datetime, x1=Event_End_Datetime, line_width=0, fillcolor=\"grey\", opacity=0.2,annotation_text=\"Distruptive Event\", annotation_position=\"inside top left\")\n",
    "    fig.update_layout(showlegend=False,title={'text':\"Time-{} Plot\".format(selected_metric), 'y':0.9,\n",
    "        'x':0.5, 'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "    fig.update_layout(font=dict(size=16))\n",
    "\n",
    "    return fig\n",
    "\n",
    "@app.callback(Output('carpet_plot','figure'),\n",
    "             [Input('Zone_Selection','value'),Input('Simulation_Scenario','value'),Input('UpperThreshold_DegHr','value'),Input('LowerThreshold_DegHr','value')\n",
    "              \n",
    "             ])\n",
    "\n",
    "def update_figure2(selected_zone1,selected_scenario1,upper_temp,lower_temp):\n",
    "\n",
    "    \n",
    "    filtered_idf_output_df_eval=idf_output_df[(idf_output_df['Zone']==selected_zone1) & (idf_output_df['FileName']==selected_scenario1)& (idf_output_df['Metric']=='Zone Air Temperature [C](Hourly)')]\n",
    "    filtered_idf_output_df_eval[\"Day\"]=filtered_idf_output_df_eval[\"DateTime\"].dt.date\n",
    "    filtered_idf_output_df_eval[\"Hour\"]=filtered_idf_output_df_eval[\"DateTime\"].dt.time\n",
    "    \n",
    "    \n",
    "    filtered_idf_output_df_eval.loc[filtered_idf_output_df_eval['value'].between(lower_temp,upper_temp),'condition'] ='Normal'\n",
    "    filtered_idf_output_df_eval.loc[filtered_idf_output_df_eval['value']<lower_temp,'condition'] ='Too Cold'\n",
    "    filtered_idf_output_df_eval.loc[filtered_idf_output_df_eval['value']>upper_temp,'condition'] ='Too Hot'\n",
    "\n",
    "    filtered_idf_output_df_eval.loc[filtered_idf_output_df_eval['condition']=='Normal','condition_num'] = 0.5\n",
    "    filtered_idf_output_df_eval.loc[filtered_idf_output_df_eval['condition']=='Too Cold','condition_num'] = 0\n",
    "    filtered_idf_output_df_eval.loc[filtered_idf_output_df_eval['condition']=='Too Hot','condition_num'] = 1\n",
    "    \n",
    "    \n",
    "    filtered_idf_output_df_eval_matrix=filtered_idf_output_df_eval.pivot(index=['Hour'],columns='Day',values='condition_num')\n",
    "    color_scheme=[(0, '#636efa'),  (0.33, '#636efa'),(0.33, '#9ACD32'), (0.66, '#9ACD32'),(0.66, '#ef553b'),  (1.00, '#ef553b')]\n",
    "    fig2=px.imshow(filtered_idf_output_df_eval_matrix,color_continuous_scale=color_scheme)\n",
    "\n",
    "    layout = px.imshow(filtered_idf_output_df_eval_matrix,color_continuous_scale=color_scheme).layout\n",
    "\n",
    "    fig2.layout.coloraxis = layout.coloraxis\n",
    "    fig2.update_layout(coloraxis_colorbar=dict(\n",
    "        title=\"Conditions\",\n",
    "        tickvals=[0.1,0.5,0.9],\n",
    "        ticktext=[\"Too Cold\",\"Normal\",\"Too Hot\"],\n",
    "        lenmode=\"pixels\", len=300))\n",
    "    fig2.update_coloraxes(cmax=1, cmin=0)\n",
    "    fig2.update_layout(title={'text':\"Zone Air Temperature Carpet Plot\", 'y':0.95,\n",
    "        'x':0.5, 'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "    \n",
    "    fig2.update_layout(font=dict(size=16))\n",
    "    \n",
    "    \n",
    "    return fig2\n",
    "\n",
    "@app.callback(Output('filtered_summary','data'),Output('filtered_summary','columns'),\n",
    "             [Input('Simulation_Scenario','value'),Input('UpperThreshold_summary','value'),Input('LowerThreshold_summary','value')\n",
    "             ])\n",
    "\n",
    "def filteredTable(selected_scenario2,upper_temp,lower_temp):\n",
    "    \n",
    "    Indicator='Zone Air Temperature [C](Hourly)'\n",
    "    Results_df=idf_output_df_original\n",
    "    Results_df_Selected=Results_df[Results_df[\"FileName\"]==selected_scenario2]\n",
    "    Results_df_Selected=Results_df_Selected.filter(like=Indicator, axis=1)\n",
    "    Results_df_Summary=Results_df_Selected.describe().T\n",
    "\n",
    "    # Analyze Results \n",
    "\n",
    "\n",
    "    ## Results summary - Max, Min of Metric based on zone\n",
    "    Results_df_Summary=Results_df_Summary.reset_index()\n",
    "    Results_df_Summary=Results_df_Summary.rename(columns={Results_df_Summary.columns[0]: 'Zone:Metric'})\n",
    "    Results_df_Summary[['Zone','Metric']] = Results_df_Summary['Zone:Metric'].str.split(':',expand=True)\n",
    "    Results_df_Summary=Results_df_Summary.set_index('Zone:Metric')\n",
    "\n",
    "    Cooling_Critical_Temp=upper_temp\n",
    "    Heating_Crtitical_Temp=lower_temp\n",
    "\n",
    "    HDH=lower_temp-Results_df_Selected\n",
    "    HDH[HDH<0]=0\n",
    "    Results_df_Summary['Underheating Degree Hours']=round(HDH.sum(),1)\n",
    "\n",
    "    CDH=Results_df_Selected-upper_temp\n",
    "    CDH[CDH<0]=0\n",
    "    Results_df_Summary['Overheating Degree Hours']=round(CDH.sum(),1)\n",
    "\n",
    "    Results_df_Summary\n",
    "\n",
    "    #Zone Selection\n",
    "\n",
    "    Analysis_Zone = pd.DataFrame()\n",
    "    Analysis_Zone_Num=5\n",
    "\n",
    "\n",
    "    ## Based on Max Indicator\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='max',ascending=False).reset_index()\n",
    "    Results_df_Summary['max']=round(Results_df_Summary['max'],1)\n",
    "    Analysis_Zone['Max_Indicator']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "\n",
    "    ## Based on Min Indicator\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='min',ascending=False).reset_index()\n",
    "    Results_df_Summary['min']=round(Results_df_Summary['min'],1)\n",
    "    Analysis_Zone['Min_Indicator']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "\n",
    "    ## Based on Max Heating Degree Hour\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='Underheating Degree Hours',ascending=False).reset_index()\n",
    "    Analysis_Zone['Underheating Degree Hours']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "    ## Based on Min Heating Degree Hour\n",
    "    Results_df_Summary_Select=Results_df_Summary.sort_values(by='Overheating Degree Hours',ascending=False).reset_index()\n",
    "    Analysis_Zone['Overheating Degree Hours']=Results_df_Summary_Select['Zone'].head(Analysis_Zone_Num)\n",
    "\n",
    "    Analysis_Zone=Analysis_Zone.melt()\n",
    "\n",
    "    Analysis_Zone=Analysis_Zone.rename(columns={\"value\": \"Zone\", \"variable\": \"Metric of Interst\"})\n",
    "\n",
    "\n",
    "    Analysis_Zone_Summary=Analysis_Zone.groupby('Zone')['Metric of Interst'].apply(','.join).reset_index()\n",
    "\n",
    "\n",
    "    Analysis_Zone_Summary=Analysis_Zone_Summary.merge(Results_df_Summary_Select, how='left', on='Zone')\n",
    "    Analysis_Zone_Summary1=Analysis_Zone_Summary.drop(columns=['mean','std','25%','50%','75%','Zone:Metric'], axis=1)\n",
    "\n",
    "    return Analysis_Zone_Summary1.to_dict('records'),[{\"name\": i, \"id\": i} for i in Analysis_Zone_Summary1.columns]\n",
    "\n",
    "@app.callback(Output('filtered_TA_summary','data'),Output('filtered_TA_summary','columns'),\n",
    "             [Input('Zone_Selection','value'),\n",
    "             Input('Simulation_Scenario','value'),\n",
    "             Input('UpperThreshold_DegHr','value'),\n",
    "             Input('LowerThreshold_DegHr','value')\n",
    "             ])\n",
    "\n",
    "\n",
    "def update_TA_summary_table(selected_zone2,selected_scenario2,upper_temp,lower_temp):\n",
    "    \n",
    "    \n",
    "    filtered_idf_output_df_eval_TA=idf_output_df[(idf_output_df['Zone']==selected_zone2) & (idf_output_df['FileName']==selected_scenario2)& (idf_output_df['Metric']=='Zone Air Temperature [C](Hourly)')]\n",
    "    filtered_idf_output_df_eval_TA[\"Day\"]=filtered_idf_output_df_eval_TA[\"DateTime\"].dt.date\n",
    "    filtered_idf_output_df_eval_TA[\"Hour\"]=filtered_idf_output_df_eval_TA[\"DateTime\"].dt.time\n",
    "    \n",
    "   \n",
    "    filtered_idf_output_df_eval_TA.loc[filtered_idf_output_df_eval_TA['value'].between(lower_temp,upper_temp),'condition'] ='Normal'\n",
    "    filtered_idf_output_df_eval_TA.loc[filtered_idf_output_df_eval_TA['value']<lower_temp,'condition'] ='Too Cold'\n",
    "    filtered_idf_output_df_eval_TA.loc[filtered_idf_output_df_eval_TA['value']>upper_temp,'condition'] ='Too Hot'\n",
    "    \n",
    "    Analysis_Zone_Summary_TA=pd.DataFrame()\n",
    "    \n",
    "    Analysis_Zone_Summary_TA.at[0,'Proportion of Year Too Hot']=round(len(filtered_idf_output_df_eval_TA[filtered_idf_output_df_eval_TA['condition']==\"Too Hot\"])/8760,2)\n",
    "    Analysis_Zone_Summary_TA.at[0,'Proportion of Year Too Cold']=round(len(filtered_idf_output_df_eval_TA[filtered_idf_output_df_eval_TA['condition']==\"Too Cold\"])/8760,2)\n",
    "    Analysis_Zone_Summary_TA.at[0,'Proportion of Year Normal']=round(len(filtered_idf_output_df_eval_TA[filtered_idf_output_df_eval_TA['condition']==\"Normal\"])/8760,2)\n",
    "\n",
    "    \n",
    "    return Analysis_Zone_Summary_TA.to_dict('records'),[{\"name\": i, \"id\": i} for i in Analysis_Zone_Summary_TA.columns]\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "                             \n",
    "if __name__=='__main__':\n",
    "    app.run_server()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
